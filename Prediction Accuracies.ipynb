{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import gzip\n",
    "from datetime import datetime, timedelta\n",
    "from statistics import mean, median\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow.keras as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, Conv2D, Dropout\n",
    "from tensorflow.keras.activations import sigmoid, tanh\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import f1_score as f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model(filename):\n",
    "    model = tf.models.load_model(filename)\n",
    "    return model\n",
    "    \n",
    "def retrieve_data(filename):\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    return df\n",
    "\n",
    "def create_classification_data(df, lookback, column):\n",
    "    rows = []\n",
    "    columns = ['Date', column] # Date and SP500_relative_change_perc_1 from t-0 are added first as target variables \n",
    "    \n",
    "    # create column names based on original with the addition of t-i where i is lookback\n",
    "    for i in range(1, lookback + 1): # starts at 1 since we do not want t-0 variables apart from 'Date' and 'SP500_relative_change_perc_1'\n",
    "        new_columns = df.columns.tolist()[1:] # starts at 1 to exclude 'Date' column\n",
    "        for x in range(len(new_columns)):\n",
    "            new_columns[x] = new_columns[x] + \"_t-\" + str(i)\n",
    "        columns = columns + new_columns\n",
    "    \n",
    "    # create lookback data\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        if i > lookback: # lookback cannot be determined for earlier rows\n",
    "            new_row = [row[1][0], row[1][1]] # add target 'Date' and 'SP500_relative_change_perc_1 '\n",
    "            for x in range(1, lookback + 1): # starts at 1 since we do not want t-0 variables apart from 'Date' and 'SP500_relative_change_perc_1'\n",
    "                add_row = df.iloc[i - x].tolist()[1:] # starts at 1 to exclude 'Date' column\n",
    "                new_row = new_row + add_row\n",
    "            rows.append(new_row)\n",
    "    df2 = pd.DataFrame(rows)\n",
    "    df2.columns = columns\n",
    "    return df2\n",
    "\n",
    "def create_train(df, year_val, year_test, column, perc_train=None):\n",
    "    # assumes years_train < year_val < year_test\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "    val = df[df['Date'].dt.year == year_val]\n",
    "    test = df[df['Date'].dt.year == year_test]\n",
    "    train = df[df['Date'].dt.year < year_val]\n",
    "    y_train = train[column]\n",
    "    x_train = train\n",
    "    return x_train\n",
    "\n",
    "\n",
    "def create_val(df, year_val, year_test, column, perc_train=None):\n",
    "    # assumes years_train < year_val < year_test\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    val = df[df['Date'].dt.year == year_val]\n",
    "    test = df[df['Date'].dt.year == year_test]\n",
    "    train = df[df['Date'].dt.year < year_val]\n",
    "    y_val = val[column]\n",
    "    x_val = val\n",
    "#     display(x_val)\n",
    "    return x_val\n",
    "\n",
    "def create_test(df, year_val, year_test, column, perc_train=None):\n",
    "    print(\"test\", len(df))\n",
    "    # assumes years_train < year_val < year_test\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "    val = df[df['Date'].dt.year == year_val]\n",
    "    test = df[df['Date'].dt.year == year_test]\n",
    "    train = df[df['Date'].dt.year < year_val]\n",
    "    y_test = test[column]\n",
    "    x_test = test\n",
    "    return x_test\n",
    "\n",
    "def create_full(df, year_val, year_test, column, perc_train=None):\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    y = df[column]\n",
    "    x = df\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(instruments, data, mode):\n",
    "    year_val = 2018\n",
    "    year_test = 2019\n",
    "    for symbol in instruments:\n",
    "        column = symbol + \"_relative_change_perc_1\"\n",
    "        ndir = symbol + \"_dir\"\n",
    "        nmag = symbol + \"_mag\"\n",
    "\n",
    "        data_dir = data[ndir]\n",
    "        data_mag = data[nmag]\n",
    "        if mode == \"train\":\n",
    "            data[ndir] = create_train(data_dir, year_val, year_test, column)\n",
    "            data[nmag] = create_train(data_mag, year_val, year_test, column)\n",
    "        if mode == \"val\":\n",
    "            data[ndir] = create_val(data_dir, year_val, year_test, column)\n",
    "            data[nmag] = create_val(data_mag, year_val, year_test, column)\n",
    "        if mode == \"test\":\n",
    "            data[ndir] = create_test(data_dir, year_val, year_test, column)\n",
    "            data[nmag] = create_test(data_mag, year_val, year_test, column)\n",
    "        if mode == \"full\":\n",
    "            data[ndir] = create_full(data_dir, year_val, year_test, column)\n",
    "            data[nmag] = create_full(data_mag, year_val, year_test, column)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models_data():\n",
    "    instruments = ['SP500', 'NASDAQ', 'US30']\n",
    "\n",
    "    model_mag_sp500 = extract_model(\"Models/SP500_LSTM_large-small_model\")\n",
    "    model_mag_us30 = extract_model(\"Models/US30_LSTM_large-small_model\")\n",
    "    model_mag_nasdaq = extract_model(\"Models/NASDAQ_LSTM_large-small_model\")\n",
    "\n",
    "    model_dir_sp500 = extract_model(\"Models/SP500_NN_up-down_model\")\n",
    "    model_dir_us30 = extract_model(\"Models/US30_NN_up-down_model\")\n",
    "    model_dir_nasdaq = extract_model(\"Models/NASDAQ_NN_up-down_model\")\n",
    "\n",
    "    models = {\n",
    "        'SP500_mag': model_mag_sp500,\n",
    "        'US30_mag': model_mag_us30,\n",
    "        'NASDAQ_mag': model_mag_nasdaq,\n",
    "        'SP500_dir': model_dir_sp500,\n",
    "        'US30_dir': model_dir_us30,\n",
    "        'NASDAQ_dir': model_dir_nasdaq\n",
    "    }\n",
    "\n",
    "    data_mag_sp500 = create_classification_data(retrieve_data(\"Dataset v3/SP500_reduced_data_20220425.csv\"), 2, 'SP500_relative_change_perc_1')\n",
    "    data_mag_us30 = create_classification_data(retrieve_data(\"Dataset v3/US30_reduced_data_20220425.csv\"), 12, 'US30_relative_change_perc_1')\n",
    "    data_mag_nasdaq = create_classification_data(retrieve_data(\"Dataset v3/NASDAQ_reduced_data_20220425.csv\"), 18, 'NASDAQ_relative_change_perc_1')\n",
    "\n",
    "    data_dir_sp500 = create_classification_data(retrieve_data(\"Dataset v3/SP500_reduced_data_20220425.csv\"), 12, 'SP500_relative_change_perc_1')\n",
    "    data_dir_us30 = create_classification_data(retrieve_data(\"Dataset v3/US30_reduced_data_20220425.csv\"), 10, 'US30_relative_change_perc_1')\n",
    "    data_dir_nasdaq = create_classification_data(retrieve_data(\"Dataset v3/NASDAQ_reduced_data_20220425.csv\"), 10, 'NASDAQ_relative_change_perc_1')\n",
    "\n",
    "    data = {\n",
    "        'SP500_mag': data_mag_sp500,\n",
    "        'US30_mag': data_mag_us30,\n",
    "        'NASDAQ_mag': data_mag_nasdaq,\n",
    "        'SP500_dir': data_dir_sp500,\n",
    "        'US30_dir': data_dir_us30,\n",
    "        'NASDAQ_dir': data_dir_nasdaq\n",
    "    }\n",
    "    return instruments, data, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 17:39:28.046944: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-10 17:39:28.047977: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "instruments, data, models = init_models_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP500 Direction Accuracy: 0.5896414342629482\n",
      "SP500 Magnitude Accuracy: 0.6852589641434262\n",
      "NASDAQ Direction Accuracy: 0.6215139442231076\n",
      "NASDAQ Magnitude Accuracy: 0.6414342629482072\n",
      "US30 Direction Accuracy: 0.5816733067729084\n",
      "US30 Magnitude Accuracy: 0.6294820717131474\n"
     ]
    }
   ],
   "source": [
    "def label_data_dir(y):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    y = list(y)\n",
    "    \n",
    "    labels = []\n",
    "    for dev in y:\n",
    "        if float(dev) >= 0:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels\n",
    "\n",
    "def label_data_mag(y):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    y = list(y)\n",
    "    for dev in y:\n",
    "        if dev >= 0:\n",
    "            positives.append(dev)\n",
    "        else:\n",
    "            negatives.append(dev)\n",
    "    med_pos = median(positives)\n",
    "    med_neg = median(negatives)\n",
    "    \n",
    "    labels = []\n",
    "    for dev in y:\n",
    "        if dev >= med_pos or dev <= med_neg:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def predict(instruments, data, models):\n",
    "    state = []\n",
    "    for symbol in instruments:\n",
    "        column = symbol + \"_relative_change_perc_1\"\n",
    "        ndir = symbol + \"_dir\"\n",
    "        nmag = symbol + \"_mag\"\n",
    "        \n",
    "        model_dir = models[ndir]\n",
    "        data_dir = data[ndir]\n",
    "        x = create_val(data_dir, 2018, 2019, column)\n",
    "        \n",
    "        y = np.asarray(label_data_dir(x[column]))\n",
    "        y = y.reshape((y.shape[0], 1))\n",
    "        \n",
    "        x = x.drop(['Date'], axis=1)\n",
    "        x = x.drop([column], axis=1)\n",
    "        x = np.asarray(x)\n",
    "        x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        \n",
    "        y = y.tolist()\n",
    "        y_pred = model_dir.predict(x)\n",
    "        y_pred = y_pred.round()\n",
    "        y_pred = y_pred.tolist()\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            y[i] = int(y[i][0])\n",
    "            y_pred[i] = int(y_pred[i][0][0])\n",
    "\n",
    "        acc = accuracy(y, y_pred)\n",
    "        print(f\"{symbol} Direction Accuracy: {acc}\")\n",
    "        \n",
    "        \n",
    "        model_mag = models[nmag]\n",
    "        data_mag = data[nmag]\n",
    "        x = create_val(data_mag, 2018, 2019, column)\n",
    "        \n",
    "        y = np.asarray(label_data_mag(x[column]))\n",
    "        y = y.reshape((y.shape[0], 1))\n",
    "        \n",
    "        x = x.drop(['Date'], axis=1)\n",
    "        x = x.drop([column], axis=1)\n",
    "        x = np.asarray(x)\n",
    "        x = x.reshape((x.shape[0], 1, x.shape[1]))\n",
    "        \n",
    "        y = y.tolist()\n",
    "        y_pred = model_mag.predict(x)\n",
    "        y_pred = y_pred.round()\n",
    "        y_pred = y_pred.tolist()\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            y[i] = int(y[i][0])\n",
    "            y_pred[i] = int(y_pred[i][0][0])\n",
    "\n",
    "        acc = accuracy(y, y_pred)\n",
    "        print(f\"{symbol} Magnitude Accuracy: {acc}\")\n",
    "        \n",
    "        \n",
    "predict(instruments, data, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
