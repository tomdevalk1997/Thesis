{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import gzip\n",
    "from datetime import datetime, timedelta\n",
    "from statistics import mean, median\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow.keras as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization, ReLU, LSTM, Conv1D, Conv2D\n",
    "from tensorflow.keras.activations import sigmoid, tanh\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import f1_score as f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(varname, filename):\n",
    "#     df = pd.read_csv(filename, index_col=0)\n",
    "    df = pd.read_csv(filename)\n",
    "#     display(df)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    return df\n",
    "\n",
    "def create_classification_data(df, lookback):\n",
    "    rows = []\n",
    "    columns = ['Date', 'SP500_relative_change_perc_1'] # Date and SP500_relative_change_perc_1 from t-0 are added first as target variables \n",
    "    \n",
    "    # create column names based on original with the addition of t-i where i is lookback\n",
    "    for i in range(1, lookback + 1): # starts at 1 since we do not want t-0 variables apart from 'Date' and 'SP500_relative_change_perc_1'\n",
    "        new_columns = df.columns.tolist()[1:] # starts at 1 to exclude 'Date' column\n",
    "        for x in range(len(new_columns)):\n",
    "            new_columns[x] = new_columns[x] + \"_t-\" + str(i)\n",
    "        columns = columns + new_columns\n",
    "    \n",
    "    # create lookback data\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        if i > lookback: # lookback cannot be determined for earlier rows\n",
    "            new_row = [row[1][0], row[1][1]] # add target 'Date' and 'SP500_relative_change_perc_1 '\n",
    "            for x in range(1, lookback + 1): # starts at 1 since we do not want t-0 variables apart from 'Date' and 'SP500_relative_change_perc_1'\n",
    "                add_row = df.iloc[i - x].tolist()[1:] # starts at 1 to exclude 'Date' column\n",
    "                new_row = new_row + add_row\n",
    "            rows.append(new_row)\n",
    "    df2 = pd.DataFrame(rows)\n",
    "    df2.columns = columns\n",
    "    return df2\n",
    "\n",
    "def create_train_val_test(df, year_val, year_test, perc_train=None):\n",
    "    if perc_train == None:\n",
    "        # assumes years_train < year_val < year_test\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        \n",
    "        val = df[df['Date'].dt.year == year_val]\n",
    "        test = df[df['Date'].dt.year == year_test]\n",
    "        train = df[df['Date'].dt.year < year_val]\n",
    "    else:\n",
    "        train = df.head(round(len(df) * perc_train))\n",
    "        val = df.tail(len(df) - len(train))\n",
    "        test = val.tail(round(0.5 * len(val)))\n",
    "        val = val.head(len(val) - len(test))\n",
    "    y_train = train['SP500_relative_change_perc_1']\n",
    "    x_train = train.drop(['SP500_relative_change_perc_1'], axis=1)\n",
    "    \n",
    "    y_val = val['SP500_relative_change_perc_1']\n",
    "    x_val = val.drop(['SP500_relative_change_perc_1'], axis=1)\n",
    "    \n",
    "    y_test = test['SP500_relative_change_perc_1']\n",
    "    x_test = test.drop(['SP500_relative_change_perc_1'], axis=1)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "def scale_data(x):\n",
    "    standard_scaler = MinMaxScaler()\n",
    "    x = x.drop([\"Date\"], axis=1)\n",
    "    x_scaled = pd.DataFrame(standard_scaler.fit_transform(x), columns=x.columns)\n",
    "    return x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(y):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    y = list(y)\n",
    "    \n",
    "    labels = []\n",
    "    for dev in y:\n",
    "        if dev >= 0:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_model(object):\n",
    "    def __init__(self, x, activation_functions, batch_sizes):        \n",
    "        self.activation_function = activation_functions[x[0]]\n",
    "        \n",
    "        self.lstm1 = x[1]\n",
    "        self.lstm2 = x[2]\n",
    "        \n",
    "        self.dense1 = x[3]\n",
    "        self.dense2 = x[4]\n",
    "\n",
    "        self.dropout1 = x[5]\n",
    "        self.dropout2 = x[6]\n",
    "        \n",
    "        self.epochs = x[7]\n",
    "        self.batch_size = batch_sizes[x[8]]\n",
    "        \n",
    "        self.lookback = x[9]\n",
    "        \n",
    "    def fit(self):\n",
    "        global df\n",
    "\n",
    "        val_year = 2018\n",
    "        test_year = 2019\n",
    "        df_class = create_classification_data(df, self.lookback)\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = create_train_val_test(df_class, val_year, test_year)\n",
    "\n",
    "\n",
    "        y_train = label_data(y_train)\n",
    "        y_val = label_data(y_val)\n",
    "        y_test = label_data(y_test)\n",
    "\n",
    "        train_date = x_train[['Date']]\n",
    "        x_train = x_train.drop(['Date'], axis=1)\n",
    "        val_date = x_val[['Date']]\n",
    "        x_val = x_val.drop(['Date'], axis=1)\n",
    "        test_date = x_test[['Date']]\n",
    "        x_test = x_test.drop(['Date'], axis=1)\n",
    "\n",
    "        x_train = np.asarray(x_train)\n",
    "        x_val = np.asarray(x_val)\n",
    "        x_test = np.asarray(x_test)\n",
    "        y_train = np.asarray(y_train)\n",
    "        y_val = np.asarray(y_val)\n",
    "        y_test = np.asarray(y_test)\n",
    "\n",
    "        x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "        x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "        x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "        y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "        y_val = y_val.reshape((y_val.shape[0], 1))\n",
    "        y_test = y_test.reshape((y_test.shape[0], 1))\n",
    "\n",
    "        tensorflow.random.set_seed(111)\n",
    "        np.random.seed(111)\n",
    "        random.seed(111)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(self.lstm1, dropout=self.dropout1, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "        if self.lstm2 > 0:\n",
    "            model.add(LSTM(self.lstm2, dropout=self.dropout2, return_sequences=True))\n",
    "        if self.dense1 > 0:\n",
    "            model.add(Dense(self.dense1, activation=self.activation_function))\n",
    "        if self.dense2 > 0:\n",
    "            model.add(Dense(self.dense2, activation=self.activation_function))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"acc\"])\n",
    "        history = model.fit(x_train, y_train, epochs=self.epochs, batch_size=self.batch_size, verbose=0, validation_data=(x_val, y_val), shuffle=False)\n",
    "        \n",
    "        val_loss = mean(history.history['val_acc'][-5:])\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fitness(val_loss):\n",
    "    fitness = val_loss\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EA(object):\n",
    "    def __init__(self, population_size, activation_functions, batch_sizes):\n",
    "        self.population_size = population_size\n",
    "        self.a = 0.2\n",
    "        self.activation_functions = activation_functions\n",
    "        self.batch_sizes = batch_sizes\n",
    "        \n",
    "    def evaluate(self, x):\n",
    "        \"\"\"\n",
    "        include in fitness function\n",
    "            relative difference between train_loss and val_loss (smaller is better)\n",
    "            number of layers (smaller is better)\n",
    "            bottleneck size (smaller is better)\n",
    "            val_loss (smaller is better)\n",
    "        \n",
    "        \"\"\"\n",
    "        lstm = LSTM_model(x, self.activation_functions, self.batch_sizes)\n",
    "        val_loss = lstm.fit()\n",
    "        fitness = calculate_fitness(val_loss)\n",
    "        return fitness\n",
    "    \n",
    "    def select_triple(self, candidate, population):\n",
    "        # select three random instances for differential evolution\n",
    "        x1, x2, x3 = np.random.choice(range(len(population))), np.random.choice(range(len(population))), np.random.choice(range(len(population)))\n",
    "        while candidate == x1 or candidate == x2 or candidate == x3 or x1 == x2 or x2 == x3 or x1 == x3:\n",
    "            # keep selecting new ones until candidate != x1 != x2 != x3\n",
    "            x1, x2, x3 = np.random.choice(range(len(population))), np.random.choice(range(len(population))), np.random.choice(range(len(population)))\n",
    "        return population[x1], population[x2], population[x3]\n",
    "    \n",
    "    def mutate(self, x1, x2, x3):\n",
    "        mutated = x1 + (self.a * (x3 - x2))\n",
    "#         print(f\"mutated {mutated}\")\n",
    "        # activation function\n",
    "        mutated[0] = round(mutated[0])\n",
    "        mutated[0] = min(mutated[0], len(self.activation_functions) - 1)\n",
    "        mutated[0] = max(0, mutated[0])\n",
    "        \n",
    "        # lstm layer 1\n",
    "        mutated[1] = round(mutated[1])\n",
    "        mutated[1] = max(1, mutated[1]) # must be at least one\n",
    "\n",
    "        # lstm layer 2\n",
    "        mutated[2] = round(mutated[2])\n",
    "        mutated[2] = max(0, mutated[2])\n",
    "\n",
    "        # dense layer 1\n",
    "        mutated[3] = round(mutated[3])\n",
    "        mutated[3] = max(4, mutated[3])\n",
    "        \n",
    "        # dense layer 2\n",
    "        mutated[4] = round(mutated[4])\n",
    "        mutated[4] = max(4, mutated[4])\n",
    "\n",
    "        # dropout lstm layer 1\n",
    "        mutated[5] = max(0.05, mutated[5])\n",
    "        mutated[5] = min(0.95, mutated[5])\n",
    "        mutated[5] = round(mutated[5],2)\n",
    "        \n",
    "        # dropout lstm layer 2\n",
    "        mutated[6] = max(0.05, mutated[6])\n",
    "        mutated[6] = min(0.95, mutated[6])\n",
    "        mutated[6] = round(mutated[6],2)\n",
    "        \n",
    "        # epochs\n",
    "        mutated[7] = round(mutated[7])\n",
    "        mutated[7] = max(6, mutated[7])\n",
    "\n",
    "        # batch size\n",
    "        mutated[8] = round(mutated[8])\n",
    "        mutated[8] = min(mutated[8], len(self.batch_sizes) - 1)\n",
    "        mutated[8] = max(0, mutated[8])\n",
    "        \n",
    "        mutated[9] = round(mutated[9])\n",
    "        mutated[9] = max(1, mutated[9])\n",
    "        \n",
    "        return mutated\n",
    "        \n",
    "    def recombine(self, candidate, mutation):\n",
    "        for i in range(candidate.shape[0]):\n",
    "            prob = np.random.randint(0, 2)\n",
    "            if prob == 1:\n",
    "                candidate[i] = mutation[i]\n",
    "        return candidate\n",
    "\n",
    "    def select(self, x_new, f_new, x_old, f_old):\n",
    "        x_cat = np.concatenate([x_new, x_old], 0)\n",
    "        f_cat = np.concatenate([f_new, f_old])\n",
    "        ind = np.argsort(f_cat)\n",
    "        x = x_cat[ind]\n",
    "        f = f_cat[ind]\n",
    "        return x[-self.population_size:], f[-self.population_size:]\n",
    "    \n",
    "    def step(self, x_old, f_old):\n",
    "        x = np.copy(x_old)\n",
    "        f = np.copy(f_old)\n",
    "        for i in tqdm(range(self.population_size), total=self.population_size):\n",
    "            # choose candidate\n",
    "            candidate = x[i]\n",
    "            # select 3 instances for differential evolution\n",
    "            x1, x2, x3 = self.select_triple(i, x)\n",
    "            # mutate 3 instances\n",
    "            mutated_triple = self.mutate(x1, x2, x3)\n",
    "            # recombine candidate with mutation\n",
    "            candidate = self.recombine(candidate, mutated_triple)\n",
    "            x[i] = candidate\n",
    "            # evaluate candidate solution\n",
    "            f_candidate = self.evaluate(candidate)\n",
    "            f[i] = f_candidate\n",
    "        # select survivors\n",
    "        x, f = self.select(x, f, x_old, f_old)\n",
    "        return x, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_population(population_size, activation_functions, batch_sizes):\n",
    "    # generate initial population\n",
    "    population = []\n",
    "    print(\"Creating initial population...\")\n",
    "    for i in tqdm(range(population_size), total=population_size):\n",
    "        activation_function = random.randint(0, len(activation_functions) - 1)\n",
    "        \n",
    "        lookback = random.randint(1, 20)\n",
    "        \n",
    "        variables = lookback * 18\n",
    "        \n",
    "        lstm1 = random.randint(1, 1.5 * variables)\n",
    "        lstm2 = random.randint(0, 1.5 * variables)\n",
    "        dense1 = random.randint(4, 1.5 * variables)\n",
    "        dense2 = random.randint(4, 1.5 * variables)\n",
    "        \n",
    "        dropout1 = round(random.uniform(0.05, 0.95),2)\n",
    "        dropout2 = round(random.uniform(0.05, 0.95),2)\n",
    "\n",
    "        epochs = random.randint(10, 500)\n",
    "        \n",
    "        batch_size = random.randint(0, len(batch_sizes) - 1)\n",
    "    \n",
    "        population.append(np.asarray([activation_function, lstm1, lstm2, dense1, dense2, dropout1, dropout2, epochs, batch_size, lookback], dtype='object'))\n",
    "    print(\"Initial population ready\")\n",
    "    return np.asarray(population)\n",
    "\n",
    "def evaluate_init_population(ea, x):\n",
    "    # evaluate initial population\n",
    "    f = []\n",
    "    print(\"Evaluating initial population...\")\n",
    "    for i in tqdm(range(x.shape[0]), total=x.shape[0]):\n",
    "        instance = x[i]\n",
    "        f.append(ea.evaluate(instance))\n",
    "    print(\"Evaluation initial population completed\")\n",
    "    return np.asarray(f)\n",
    "\n",
    "def print_best(x, activation_functions, batch_sizes, fitness):\n",
    "    print(f\"\\nMost suitable parameters -- Accuracy of {fitness}:\")\n",
    "    print(f\"\\tActivation function:           \\t{activation_functions[x[0]]}\")\n",
    "    print(f\"\\tLSTM nodes layer 1:            \\t{x[1]}\")\n",
    "    print(f\"\\tLSTM nodes layer 2:            \\t{x[2]}\")\n",
    "    print(f\"\\tDense nodes layer 1:           \\t{x[3]}\")\n",
    "    print(f\"\\tDense nodes layer 2:           \\t{x[4]}\")\n",
    "    print(f\"\\tDropout LSTM layer 1:          \\t{x[5]}\")\n",
    "    print(f\"\\tDropout LSTM layer 2:          \\t{x[6]}\")\n",
    "    print(f\"\\tEpochs trained:                \\t{x[7]}\")\n",
    "    print(f\"\\tBatch Size:                    \\t{batch_sizes[x[8]]}\")\n",
    "    print(f\"\\tLookback:                      \\t{x[9]}\")\n",
    "\n",
    "def plot_convergence(f_best):\n",
    "    fig1 = make_subplots(rows=1, cols=1, specs=[[{'type':'xy'}]])\n",
    "    \n",
    "    x_values = []\n",
    "    for i in range(len(f_best)):\n",
    "        x_values.append(i)\n",
    "    fig1.add_trace(go.Scatter(x=x_values, y=f_best, mode=\"lines\"), row=1, col=1)\n",
    "\n",
    "    fig1.update_layout(\n",
    "        title = f'Validation Accuracy Over LSTM Tuning Generations', \n",
    "        xaxis1 = dict(title_text = 'Generation'),\n",
    "        yaxis1 = dict(title_text = \"Validation Accuracy\")\n",
    "    )\n",
    "    fig1.write_image(\"Plots/opt lstm 5 binary updown SP500 all.png\")\n",
    "    fig1.show()\n",
    "\n",
    "def validate_best(x, ea):\n",
    "    print(\"\\nValidating solution...\")\n",
    "    ea.evaluate(x)\n",
    "    print(\"Solution validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 44971.09it/s]\n",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SP500_AD_MACD_12_26', 'SP500_AD_oscillator', 'SP500_ATR10', 'SP500_ATR20', 'SP500_ATR5', 'SP500_ATR50', 'SP500_EMA10', 'SP500_EMA20', 'SP500_EMA5', 'SP500_EMA50', 'SP500_F_Volume', 'SP500_F_relative_change_perc_1', 'SP500_F_relative_change_perc_10', 'SP500_F_relative_change_perc_20', 'SP500_F_relative_change_perc_5', 'SP500_F_relative_change_perc_50', 'SP500_MA10', 'SP500_MA20', 'SP500_MA5', 'SP500_MA50', 'SP500_OBV', 'SP500_OBV_stdev_10', 'SP500_OBV_stdev_20', 'SP500_OBV_stdev_5', 'SP500_OBV_stdev_50', 'SP500_RSI_14', 'SP500_RSI_28', 'SP500_Volume', 'SP500_bollinger_high_10', 'SP500_bollinger_high_20', 'SP500_bollinger_high_5', 'SP500_bollinger_high_50', 'SP500_bollinger_low_10', 'SP500_bollinger_low_20', 'SP500_bollinger_low_5', 'SP500_bollinger_low_50', 'SP500_bollinger_middle_10', 'SP500_bollinger_middle_20', 'SP500_bollinger_middle_5', 'SP500_bollinger_middle_50', 'SP500_disparity_10', 'SP500_disparity_20', 'SP500_disparity_5', 'SP500_disparity_50', 'SP500_momentum_16', 'SP500_momentum_4', 'SP500_momentum_8', 'SP500_relative_change_perc_1', 'SP500_relative_change_perc_10', 'SP500_relative_change_perc_20', 'SP500_relative_change_perc_5', 'SP500_relative_change_perc_50', 'SP500_stochastic_D_10_10', 'SP500_stochastic_D_20_20', 'SP500_stochastic_D_50_50', 'SP500_stochastic_D_5_5', 'SP500_stochastic_K_10', 'SP500_stochastic_K_20', 'SP500_stochastic_K_5', 'SP500_stochastic_K_50', 'SP500_week_high_1', 'SP500_week_high_10', 'SP500_week_high_52', 'SP500_week_low_1', 'SP500_week_low_10', 'SP500_week_low_52', 'SP500_williams_R_10', 'SP500_williams_R_20', 'SP500_williams_R_5', 'SP500_williams_R_50']\n",
      "Creating initial population...\n",
      "Initial population ready\n",
      "Evaluating initial population...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 11:21:15.018120: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-02 11:21:15.018603: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-02 11:21:15.545311: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 30/30 [1:28:03<00:00, 176.11s/it]\n",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation initial population completed\n",
      "--> STARTING EVOLUTION\n",
      "Generation: 0\tBest fitness: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 30/30 [3:32:42<00:00, 425.41s/it]\n",
      "  0%|                                                                                           | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 466 386 178 69 0.85 0.41 388 2 18]\n",
      " [1 466 137 153 69 0.59 0.06 402 1 18]\n",
      " [1 466 155 132 69 0.19 0.06 459 0 18]\n",
      " [0 466 391 54 69 0.52 0.06 56 2 18]\n",
      " [1 466 181 204 69 0.37 0.06 403 0 18]\n",
      " [0 19 7 231 311 0.68 0.42 321 1 12]\n",
      " [0 466 35 10 69 0.38 0.05 81 0 18]\n",
      " [1 466 62 35 69 0.5 0.05 98 2 18]\n",
      " [1 466 428 318 69 0.35 0.05 204 2 18]\n",
      " [1 466 294 452 69 0.46 0.05 487 0 18]\n",
      " [0 466 65 16 69 0.59 0.05 112 0 18]\n",
      " [1 466 7 4 69 0.47 0.05 491 2 18]\n",
      " [0 501 7 500 38 0.68 0.41 321 1 19]\n",
      " [0 192 7 304 157 0.68 0.41 321 1 17]\n",
      " [0 466 0 39 69 0.77 0.41 356 1 18]\n",
      " [1 199 47 147 106 0.43 0.14 59 2 9]\n",
      " [1 466 91 39 69 0.58 0.41 445 2 18]\n",
      " [0 466 103 427 69 0.77 0.41 313 2 18]\n",
      " [1 466 168 99 69 0.8 0.41 451 2 18]\n",
      " [1 466 248 312 69 0.17 0.41 271 0 18]\n",
      " [1 466 118 134 69 0.3 0.41 435 2 18]\n",
      " [1 466 53 24 69 0.59 0.41 159 1 18]\n",
      " [0 466 105 45 69 0.29 0.41 362 1 18]\n",
      " [1 466 32 124 69 0.52 0.41 473 0 18]\n",
      " [0 466 2 7 69 0.85 0.41 257 1 18]\n",
      " [1 466 10 86 69 0.47 0.41 415 2 18]\n",
      " [1 466 169 131 69 0.11 0.41 390 0 18]\n",
      " [0 466 123 75 69 0.21 0.41 446 0 18]\n",
      " [1 92 386 178 301 0.85 0.49 388 2 16]\n",
      " [0 322 61 262 282 0.89 0.79 474 0 14]]\n",
      "Generation: 1\tBest fitness: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▎                                                                          | 2/30 [14:51<3:34:50, 460.36s/it]"
     ]
    }
   ],
   "source": [
    "df = retrieve_data(\"SP500\", \"Dataset v3/SP500_combined_data_20220422.csv\")\n",
    "cols = []\n",
    "relevant = 'SP500'\n",
    "for col in df.columns.tolist():\n",
    "    if col[:len(relevant)] == relevant:\n",
    "        cols.append(col)\n",
    "        \n",
    "print(cols)\n",
    "\n",
    "population_size = 30\n",
    "generations = 30\n",
    "activation_functions = ['sigmoid', 'tanh']\n",
    "batch_sizes = [64, 128, 256]\n",
    "\n",
    "ea = EA(population_size, activation_functions, batch_sizes)\n",
    "x = init_population(population_size, activation_functions, batch_sizes)\n",
    "f = evaluate_init_population(ea, x)\n",
    "\n",
    "populations = []\n",
    "populations.append(x)\n",
    "f_best = [f.max()]\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "print(\"--> STARTING EVOLUTION\")\n",
    "early_stop = 0\n",
    "for i in range(generations):\n",
    "    print(f'Generation: {i}\\tBest fitness: {f.max()}')\n",
    "    x, f = ea.step(x, f)\n",
    "    print(x)\n",
    "    populations.append(x)\n",
    "\n",
    "    if f.max() > f_best[-1]:\n",
    "        f_best.append(f.max())\n",
    "        early_stop = 0\n",
    "    else:\n",
    "        f_best.append(f_best[-1])\n",
    "        early_stop += 1\n",
    "    if early_stop == 5:\n",
    "        print(\"Early stop triggered at generation {i} after not improving fitness for three generations\")\n",
    "        break\n",
    "print(\"--> EVOLUTION FINISHED\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "evolution_time = end_time - start_time\n",
    "evolution_time_seconds = evolution_time.total_seconds()\n",
    "print(f\"\\nElapsed time in minutes: {evolution_time_seconds/60}\")\n",
    "\n",
    "print(f)\n",
    "print(f.min())\n",
    "index_best_parameters = np.where(f == f.max())[0][0]\n",
    "print(index_best_parameters)\n",
    "print_best(x[index_best_parameters], activation_functions, batch_sizes, f.max())\n",
    "validate_best(x[index_best_parameters], ea)\n",
    "plot_convergence(f_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(f_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
