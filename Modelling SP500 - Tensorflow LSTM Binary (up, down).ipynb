{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import gzip\n",
    "from datetime import datetime, timedelta\n",
    "from statistics import mean, median\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow.keras as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization, ReLU, LSTM, Conv1D, Conv2D\n",
    "from tensorflow.keras.activations import sigmoid, tanh\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import f1_score as f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_data(varname, filename):\n",
    "#     df = pd.read_csv(filename, index_col=0)\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    return df\n",
    "\n",
    "def create_classification_data(df, lookback, features=[]):\n",
    "    rows = []\n",
    "    \n",
    "    if features != []:\n",
    "        df = df[features].copy()\n",
    "    columns = ['Date', 'SP500_relative_change_perc_1'] # Date and SP500_relative_change_perc_1 from t-0 are added first as target variables \n",
    "    \n",
    "    # create column names based on original with the addition of t-i where i is lookback\n",
    "    for i in range(1, lookback + 1): # starts at 1 since we do not want t-0 variables apart from 'Date' and 'SP500_relative_change_perc_1'\n",
    "        new_columns = df.columns.tolist()[1:] # starts at 1 to exclude 'Date' column\n",
    "        for x in range(len(new_columns)):\n",
    "            new_columns[x] = new_columns[x] + \"_t-\" + str(i)\n",
    "        columns = columns + new_columns\n",
    "    \n",
    "    # create lookback data\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        if i > lookback: # lookback cannot be determined for earlier rows\n",
    "            new_row = [row[1][0], row[1][1]] # add target 'Date' and 'SP500_relative_change_perc_1 '\n",
    "            for x in range(1, lookback + 1): # starts at 1 since we do not want t-0 variables apart from 'Date' and 'SP500_relative_change_perc_1'\n",
    "                add_row = df.iloc[i - x].tolist()[1:] # starts at 1 to exclude 'Date' column\n",
    "                new_row = new_row + add_row\n",
    "            rows.append(new_row)\n",
    "    df2 = pd.DataFrame(rows)\n",
    "    df2.columns = columns\n",
    "    return df2\n",
    "\n",
    "def create_train_val_test(df, year_val, year_test, perc_train=None):\n",
    "    if perc_train == None:\n",
    "        # assumes years_train < year_val < year_test\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        \n",
    "        val = df[df['Date'].dt.year == year_val]\n",
    "        test = df[df['Date'].dt.year == year_test]\n",
    "        train = df[df['Date'].dt.year < year_val]\n",
    "    else:\n",
    "        train = df.head(round(len(df) * perc_train))\n",
    "        val = df.tail(len(df) - len(train))\n",
    "        test = val.tail(round(0.5 * len(val)))\n",
    "        val = df.head(len(val) - len(test))\n",
    "    y_train = train['SP500_relative_change_perc_1']\n",
    "    x_train = train.drop(['SP500_relative_change_perc_1'], axis=1)\n",
    "    \n",
    "    y_val = val['SP500_relative_change_perc_1']\n",
    "    x_val = val.drop(['SP500_relative_change_perc_1'], axis=1)\n",
    "    \n",
    "    y_test = test['SP500_relative_change_perc_1']\n",
    "    x_test = test.drop(['SP500_relative_change_perc_1'], axis=1)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "def scale_data(x):\n",
    "    standard_scaler = MinMaxScaler()\n",
    "    x = x.drop([\"Date\"], axis=1)\n",
    "    x_scaled = pd.DataFrame(standard_scaler.fit_transform(x), columns=x.columns)\n",
    "    return x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'Date', 'SP500_relative_change_perc_1', 'SP500_F_relative_change_perc_1', 'Gold_F_relative_change_perc_1', 'Silver_F_relative_change_perc_1', 'Copper_F_relative_change_perc_1', 'SP500_williams_R_5', 'SP500_williams_R_10', 'SP500_williams_R_20', 'SP500_williams_R_50', 'SP500_AD_MACD_12_26', 'SP500_stochastic_D_5_5', 'SP500_momentum_8', 'SP500_momentum_16', 'SP500_AD_oscillator', 'SP500_stochastic_K_5', 'SP500_stochastic_K_10', 'SP500_stochastic_K_20', 'SP500_stochastic_K_50']\n"
     ]
    }
   ],
   "source": [
    "lookback = 3\n",
    "val_year = 2018\n",
    "test_year = 2019\n",
    "\n",
    "files = {\n",
    "    # varname: filename\n",
    "    \"S&P500\": \"Dataset v3/SP500_reduced_data_20220425.csv\",\n",
    "#     \"S&P500\": \"Dataset v3/SP500_combined_data_20220422.csv\"\n",
    "}\n",
    "\n",
    "for file in files:\n",
    "    df = retrieve_data(file, files[file])\n",
    "print(df.columns.tolist())\n",
    "# df = create_classification_data(df, lookback)\n",
    "\n",
    "# x_train, y_train, x_val, y_val, x_test, y_test = create_train_val_test(df, val_year, test_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       575930000.0\n",
       "1       343150000.0\n",
       "2       445270000.0\n",
       "3       483770000.0\n",
       "4       347500000.0\n",
       "5       485580000.0\n",
       "6       393570000.0\n",
       "7       602150000.0\n",
       "8       735530000.0\n",
       "9       874780000.0\n",
       "10      874100000.0\n",
       "11      526960000.0\n",
       "12      438360000.0\n",
       "13      433310000.0\n",
       "14      363550000.0\n",
       "15      382430000.0\n",
       "16      469600000.0\n",
       "17      422690000.0\n",
       "18      394240000.0\n",
       "19      395810000.0\n",
       "20      423180000.0\n",
       "21      341770000.0\n",
       "22      387480000.0\n",
       "23      300440000.0\n",
       "24      355490000.0\n",
       "25      445070000.0\n",
       "26      440160000.0\n",
       "27      306050000.0\n",
       "28      524380000.0\n",
       "29      431150000.0\n",
       "           ...     \n",
       "2107     94550000.0\n",
       "2108     87600000.0\n",
       "2109     65050000.0\n",
       "2110    100530000.0\n",
       "2111    102360000.0\n",
       "2112     56110000.0\n",
       "2113     82870000.0\n",
       "2114    105720000.0\n",
       "2115    166670000.0\n",
       "2116    166110000.0\n",
       "2117    159040000.0\n",
       "2118    130170000.0\n",
       "2119    109400000.0\n",
       "2120    114240000.0\n",
       "2121    102690000.0\n",
       "2122     93420000.0\n",
       "2123    141100000.0\n",
       "2124     77640000.0\n",
       "2125     95270000.0\n",
       "2126     81910000.0\n",
       "2127    160680000.0\n",
       "2128    117680000.0\n",
       "2129    109750000.0\n",
       "2130     93900000.0\n",
       "2131     83800000.0\n",
       "2132     65400000.0\n",
       "2133    132740000.0\n",
       "2134     85990000.0\n",
       "2135     65920000.0\n",
       "2136    104000000.0\n",
       "Name: SP500_relative_change_perc_1, Length: 2137, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(y):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    y = list(y)\n",
    "#     for dev in y:\n",
    "#         if dev >= 0:\n",
    "#             positives.append(dev)\n",
    "#         else:\n",
    "#             negatives.append(dev)\n",
    "#     med_pos = median(positives)\n",
    "#     med_neg = median(negatives)\n",
    "    \n",
    "    labels = []\n",
    "    for dev in y:\n",
    "        if dev >= 0:\n",
    "            labels.append(1)\n",
    "#             if dev >= med_pos:\n",
    "#                 labels.append(2)\n",
    "#             else:\n",
    "#                 labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "#             if dev <= med_neg:\n",
    "#                 labels.append(-2)\n",
    "#             else:\n",
    "#                 labels.append(-1)\n",
    "    return labels\n",
    "# \n",
    "y_train = label_data(y_train)\n",
    "y_val = label_data(y_val)\n",
    "y_test = label_data(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline training set\n",
      "\tDistribution: [0, 2126]\n",
      "\tRandom baseline accuracy (majority class): 1.0\n",
      "Random baseline validation set\n",
      "\tDistribution: [0, 251]\n",
      "\tRandom baseline accuracy (majority class): 1.0\n",
      "Random baseline test set\n",
      "\tDistribution: [0, 251]\n",
      "\tRandom baseline accuracy (majority class): 1.0\n"
     ]
    }
   ],
   "source": [
    "def random_baseline(y):\n",
    "    counts = [0, 0]\n",
    "    for i in y:\n",
    "        if i == 0:\n",
    "            counts[0] = counts[0] + 1\n",
    "        elif i == 1:\n",
    "            counts[1] = counts[1] + 1\n",
    "    print(f\"\\tDistribution: {counts}\")\n",
    "    print(f\"\\tRandom baseline accuracy (majority class): {counts[np.argmax(np.asarray(counts))]/ len(y)}\")\n",
    "    \n",
    "print(\"Random baseline training set\")\n",
    "random_baseline(y_train)\n",
    "print(\"Random baseline validation set\")\n",
    "random_baseline(y_val)\n",
    "print(\"Random baseline test set\")\n",
    "random_baseline(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date = x_train[['Date']]\n",
    "x_train = x_train.drop(['Date'], axis=1)\n",
    "\n",
    "val_date = x_val[['Date']]\n",
    "x_val = x_val.drop(['Date'], axis=1)\n",
    "\n",
    "test_date = x_test[['Date']]\n",
    "x_test = x_test.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2126, 4200) (2126,)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "x_val = np.asarray(x_val)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_val = np.asarray(y_val)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "x_test = x_test.reshape((x_test.shape[0], 1, x_test.shape[1]))\n",
    "\n",
    "# y_train = to_categorical(y_train, 1)\n",
    "# y_val = to_categorical(y_val, 1)\n",
    "# y_test = to_categorical(y_test, 1)\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "y_val = y_val.reshape((y_val.shape[0], 1))\n",
    "y_test = y_test.reshape((y_test.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2126, 1, 4200) (2126, 1)\n",
      "(251, 1, 4200) (251, 1)\n",
      "(252, 1, 4200) (252, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/474\n",
      "34/34 [==============================] - 11s 106ms/step - loss: 0.1811 - acc: 0.8815 - val_loss: 4.6244e-04 - val_acc: 1.0000\n",
      "Epoch 2/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 4.1337e-04 - acc: 1.0000 - val_loss: 3.3251e-04 - val_acc: 1.0000\n",
      "Epoch 3/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.3320e-04 - acc: 1.0000 - val_loss: 3.0434e-04 - val_acc: 1.0000\n",
      "Epoch 4/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 3.0718e-04 - acc: 1.0000 - val_loss: 2.7886e-04 - val_acc: 1.0000\n",
      "Epoch 5/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.8242e-04 - acc: 1.0000 - val_loss: 2.5425e-04 - val_acc: 1.0000\n",
      "Epoch 6/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.5856e-04 - acc: 1.0000 - val_loss: 2.3102e-04 - val_acc: 1.0000\n",
      "Epoch 7/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 2.3630e-04 - acc: 1.0000 - val_loss: 2.0938e-04 - val_acc: 1.0000\n",
      "Epoch 8/474\n",
      "34/34 [==============================] - 2s 61ms/step - loss: 2.1634e-04 - acc: 1.0000 - val_loss: 1.8943e-04 - val_acc: 1.0000\n",
      "Epoch 9/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 1.9708e-04 - acc: 1.0000 - val_loss: 1.7116e-04 - val_acc: 1.0000\n",
      "Epoch 10/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 1.8000e-04 - acc: 1.0000 - val_loss: 1.5440e-04 - val_acc: 1.0000\n",
      "Epoch 11/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 1.6460e-04 - acc: 1.0000 - val_loss: 1.3906e-04 - val_acc: 1.0000\n",
      "Epoch 12/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.4997e-04 - acc: 1.0000 - val_loss: 1.2507e-04 - val_acc: 1.0000\n",
      "Epoch 13/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.3798e-04 - acc: 1.0000 - val_loss: 1.1241e-04 - val_acc: 1.0000\n",
      "Epoch 14/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.2659e-04 - acc: 1.0000 - val_loss: 1.0093e-04 - val_acc: 1.0000\n",
      "Epoch 15/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.1566e-04 - acc: 1.0000 - val_loss: 9.0520e-05 - val_acc: 1.0000\n",
      "Epoch 16/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.0640e-04 - acc: 1.0000 - val_loss: 8.1190e-05 - val_acc: 1.0000\n",
      "Epoch 17/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 9.7242e-05 - acc: 1.0000 - val_loss: 7.2761e-05 - val_acc: 1.0000\n",
      "Epoch 18/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 8.9026e-05 - acc: 1.0000 - val_loss: 6.5177e-05 - val_acc: 1.0000\n",
      "Epoch 19/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 8.3153e-05 - acc: 1.0000 - val_loss: 5.8562e-05 - val_acc: 1.0000\n",
      "Epoch 20/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 7.6126e-05 - acc: 1.0000 - val_loss: 5.2748e-05 - val_acc: 1.0000\n",
      "Epoch 21/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 7.1024e-05 - acc: 1.0000 - val_loss: 4.7717e-05 - val_acc: 1.0000\n",
      "Epoch 22/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 6.5524e-05 - acc: 1.0000 - val_loss: 4.3361e-05 - val_acc: 1.0000\n",
      "Epoch 23/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 6.1209e-05 - acc: 1.0000 - val_loss: 3.9484e-05 - val_acc: 1.0000\n",
      "Epoch 24/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 5.5529e-05 - acc: 1.0000 - val_loss: 3.6028e-05 - val_acc: 1.0000\n",
      "Epoch 25/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 5.2247e-05 - acc: 1.0000 - val_loss: 3.3043e-05 - val_acc: 1.0000\n",
      "Epoch 26/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 4.9376e-05 - acc: 1.0000 - val_loss: 3.0416e-05 - val_acc: 1.0000\n",
      "Epoch 27/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 4.5982e-05 - acc: 1.0000 - val_loss: 2.8127e-05 - val_acc: 1.0000\n",
      "Epoch 28/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 4.3316e-05 - acc: 1.0000 - val_loss: 2.6081e-05 - val_acc: 1.0000\n",
      "Epoch 29/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 4.0589e-05 - acc: 1.0000 - val_loss: 2.4286e-05 - val_acc: 1.0000\n",
      "Epoch 30/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 3.8872e-05 - acc: 1.0000 - val_loss: 2.2676e-05 - val_acc: 1.0000\n",
      "Epoch 31/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.6814e-05 - acc: 1.0000 - val_loss: 2.1249e-05 - val_acc: 1.0000\n",
      "Epoch 32/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.4026e-05 - acc: 1.0000 - val_loss: 1.9968e-05 - val_acc: 1.0000\n",
      "Epoch 33/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 3.2704e-05 - acc: 1.0000 - val_loss: 1.8810e-05 - val_acc: 1.0000\n",
      "Epoch 34/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 3.0850e-05 - acc: 1.0000 - val_loss: 1.7780e-05 - val_acc: 1.0000\n",
      "Epoch 35/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 3.0360e-05 - acc: 1.0000 - val_loss: 1.6838e-05 - val_acc: 1.0000\n",
      "Epoch 36/474\n",
      "34/34 [==============================] - 2s 61ms/step - loss: 2.7752e-05 - acc: 1.0000 - val_loss: 1.5986e-05 - val_acc: 1.0000\n",
      "Epoch 37/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.7235e-05 - acc: 1.0000 - val_loss: 1.5212e-05 - val_acc: 1.0000\n",
      "Epoch 38/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.5887e-05 - acc: 1.0000 - val_loss: 1.4495e-05 - val_acc: 1.0000\n",
      "Epoch 39/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.4646e-05 - acc: 1.0000 - val_loss: 1.3837e-05 - val_acc: 1.0000\n",
      "Epoch 40/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.4079e-05 - acc: 1.0000 - val_loss: 1.3233e-05 - val_acc: 1.0000\n",
      "Epoch 41/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.2902e-05 - acc: 1.0000 - val_loss: 1.2667e-05 - val_acc: 1.0000\n",
      "Epoch 42/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.1561e-05 - acc: 1.0000 - val_loss: 1.2146e-05 - val_acc: 1.0000\n",
      "Epoch 43/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 2.1178e-05 - acc: 1.0000 - val_loss: 1.1665e-05 - val_acc: 1.0000\n",
      "Epoch 44/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 2.0673e-05 - acc: 1.0000 - val_loss: 1.1213e-05 - val_acc: 1.0000\n",
      "Epoch 45/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.9250e-05 - acc: 1.0000 - val_loss: 1.0788e-05 - val_acc: 1.0000\n",
      "Epoch 46/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.9176e-05 - acc: 1.0000 - val_loss: 1.0398e-05 - val_acc: 1.0000\n",
      "Epoch 47/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 1.8563e-05 - acc: 1.0000 - val_loss: 1.0035e-05 - val_acc: 1.0000\n",
      "Epoch 48/474\n",
      "34/34 [==============================] - 2s 60ms/step - loss: 1.7361e-05 - acc: 1.0000 - val_loss: 9.6840e-06 - val_acc: 1.0000\n",
      "Epoch 49/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 1.7288e-05 - acc: 1.0000 - val_loss: 9.3603e-06 - val_acc: 1.0000\n",
      "Epoch 50/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.6356e-05 - acc: 1.0000 - val_loss: 9.0459e-06 - val_acc: 1.0000\n",
      "Epoch 51/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 1.5647e-05 - acc: 1.0000 - val_loss: 8.7484e-06 - val_acc: 1.0000\n",
      "Epoch 52/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.5598e-05 - acc: 1.0000 - val_loss: 8.4758e-06 - val_acc: 1.0000\n",
      "Epoch 53/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.5417e-05 - acc: 1.0000 - val_loss: 8.2138e-06 - val_acc: 1.0000\n",
      "Epoch 54/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.4352e-05 - acc: 1.0000 - val_loss: 7.9771e-06 - val_acc: 1.0000\n",
      "Epoch 55/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.4056e-05 - acc: 1.0000 - val_loss: 7.7391e-06 - val_acc: 1.0000\n",
      "Epoch 56/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.3742e-05 - acc: 1.0000 - val_loss: 7.5218e-06 - val_acc: 1.0000\n",
      "Epoch 57/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.3412e-05 - acc: 1.0000 - val_loss: 7.3162e-06 - val_acc: 1.0000\n",
      "Epoch 58/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.3252e-05 - acc: 1.0000 - val_loss: 7.1094e-06 - val_acc: 1.0000\n",
      "Epoch 59/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.2590e-05 - acc: 1.0000 - val_loss: 6.9222e-06 - val_acc: 1.0000\n",
      "Epoch 60/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 1.2223e-05 - acc: 1.0000 - val_loss: 6.7401e-06 - val_acc: 1.0000\n",
      "Epoch 61/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.2028e-05 - acc: 1.0000 - val_loss: 6.5724e-06 - val_acc: 1.0000\n",
      "Epoch 62/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.1336e-05 - acc: 1.0000 - val_loss: 6.4015e-06 - val_acc: 1.0000\n",
      "Epoch 63/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.1322e-05 - acc: 1.0000 - val_loss: 6.2469e-06 - val_acc: 1.0000\n",
      "Epoch 64/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.0953e-05 - acc: 1.0000 - val_loss: 6.0928e-06 - val_acc: 1.0000\n",
      "Epoch 65/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.0762e-05 - acc: 1.0000 - val_loss: 5.9493e-06 - val_acc: 1.0000\n",
      "Epoch 66/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.0276e-05 - acc: 1.0000 - val_loss: 5.8035e-06 - val_acc: 1.0000\n",
      "Epoch 67/474\n",
      "34/34 [==============================] - 2s 61ms/step - loss: 9.9365e-06 - acc: 1.0000 - val_loss: 5.6715e-06 - val_acc: 1.0000\n",
      "Epoch 68/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 9.8622e-06 - acc: 1.0000 - val_loss: 5.5442e-06 - val_acc: 1.0000\n",
      "Epoch 69/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 9.5847e-06 - acc: 1.0000 - val_loss: 5.4155e-06 - val_acc: 1.0000\n",
      "Epoch 70/474\n",
      "34/34 [==============================] - 2s 58ms/step - loss: 9.4778e-06 - acc: 1.0000 - val_loss: 5.2968e-06 - val_acc: 1.0000\n",
      "Epoch 71/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 9.2392e-06 - acc: 1.0000 - val_loss: 5.1756e-06 - val_acc: 1.0000\n",
      "Epoch 72/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 8.8626e-06 - acc: 1.0000 - val_loss: 5.0633e-06 - val_acc: 1.0000\n",
      "Epoch 73/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 8.8977e-06 - acc: 1.0000 - val_loss: 4.9515e-06 - val_acc: 1.0000\n",
      "Epoch 74/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 8.5990e-06 - acc: 1.0000 - val_loss: 4.8494e-06 - val_acc: 1.0000\n",
      "Epoch 75/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 8.3783e-06 - acc: 1.0000 - val_loss: 4.7541e-06 - val_acc: 1.0000\n",
      "Epoch 76/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 8.0952e-06 - acc: 1.0000 - val_loss: 4.6570e-06 - val_acc: 1.0000\n",
      "Epoch 77/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 7.7955e-06 - acc: 1.0000 - val_loss: 4.5613e-06 - val_acc: 1.0000\n",
      "Epoch 78/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 7.8806e-06 - acc: 1.0000 - val_loss: 4.4689e-06 - val_acc: 1.0000\n",
      "Epoch 79/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 7.8965e-06 - acc: 1.0000 - val_loss: 4.3804e-06 - val_acc: 1.0000\n",
      "Epoch 80/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 7.6461e-06 - acc: 1.0000 - val_loss: 4.2932e-06 - val_acc: 1.0000\n",
      "Epoch 81/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 7.4585e-06 - acc: 1.0000 - val_loss: 4.2039e-06 - val_acc: 1.0000\n",
      "Epoch 82/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 7.2593e-06 - acc: 1.0000 - val_loss: 4.1237e-06 - val_acc: 1.0000\n",
      "Epoch 83/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 7.1271e-06 - acc: 1.0000 - val_loss: 4.0458e-06 - val_acc: 1.0000\n",
      "Epoch 84/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 7.1104e-06 - acc: 1.0000 - val_loss: 3.9691e-06 - val_acc: 1.0000\n",
      "Epoch 85/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 6.5717e-06 - acc: 1.0000 - val_loss: 3.8895e-06 - val_acc: 1.0000\n",
      "Epoch 86/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 6.5295e-06 - acc: 1.0000 - val_loss: 3.8114e-06 - val_acc: 1.0000\n",
      "Epoch 87/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 6.7186e-06 - acc: 1.0000 - val_loss: 3.7463e-06 - val_acc: 1.0000\n",
      "Epoch 88/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 6.3235e-06 - acc: 1.0000 - val_loss: 3.6755e-06 - val_acc: 1.0000\n",
      "Epoch 89/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 6.1955e-06 - acc: 1.0000 - val_loss: 3.6138e-06 - val_acc: 1.0000\n",
      "Epoch 90/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 6.0696e-06 - acc: 1.0000 - val_loss: 3.5459e-06 - val_acc: 1.0000\n",
      "Epoch 91/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 5.8364e-06 - acc: 1.0000 - val_loss: 3.4794e-06 - val_acc: 1.0000\n",
      "Epoch 92/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 5.7798e-06 - acc: 1.0000 - val_loss: 3.4179e-06 - val_acc: 1.0000\n",
      "Epoch 93/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 5.6810e-06 - acc: 1.0000 - val_loss: 3.3576e-06 - val_acc: 1.0000\n",
      "Epoch 94/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 5.5932e-06 - acc: 1.0000 - val_loss: 3.2984e-06 - val_acc: 1.0000\n",
      "Epoch 95/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 5.3502e-06 - acc: 1.0000 - val_loss: 3.2377e-06 - val_acc: 1.0000\n",
      "Epoch 96/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 5.2891e-06 - acc: 1.0000 - val_loss: 3.1861e-06 - val_acc: 1.0000\n",
      "Epoch 97/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 5.3894e-06 - acc: 1.0000 - val_loss: 3.1270e-06 - val_acc: 1.0000\n",
      "Epoch 98/474\n",
      "34/34 [==============================] - 2s 63ms/step - loss: 5.2775e-06 - acc: 1.0000 - val_loss: 3.0752e-06 - val_acc: 1.0000\n",
      "Epoch 99/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 5.2575e-06 - acc: 1.0000 - val_loss: 3.0216e-06 - val_acc: 1.0000\n",
      "Epoch 100/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 5.0574e-06 - acc: 1.0000 - val_loss: 2.9726e-06 - val_acc: 1.0000\n",
      "Epoch 101/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 4.7447e-06 - acc: 1.0000 - val_loss: 2.9175e-06 - val_acc: 1.0000\n",
      "Epoch 102/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 4.6793e-06 - acc: 1.0000 - val_loss: 2.8746e-06 - val_acc: 1.0000\n",
      "Epoch 103/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 4.7249e-06 - acc: 1.0000 - val_loss: 2.8247e-06 - val_acc: 1.0000\n",
      "Epoch 104/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 4.5851e-06 - acc: 1.0000 - val_loss: 2.7760e-06 - val_acc: 1.0000\n",
      "Epoch 105/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 4.4295e-06 - acc: 1.0000 - val_loss: 2.7304e-06 - val_acc: 1.0000\n",
      "Epoch 106/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 4.4726e-06 - acc: 1.0000 - val_loss: 2.6855e-06 - val_acc: 1.0000\n",
      "Epoch 107/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 4.4849e-06 - acc: 1.0000 - val_loss: 2.6402e-06 - val_acc: 1.0000\n",
      "Epoch 108/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 4.3091e-06 - acc: 1.0000 - val_loss: 2.5991e-06 - val_acc: 1.0000\n",
      "Epoch 109/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 4.4002e-06 - acc: 1.0000 - val_loss: 2.5549e-06 - val_acc: 1.0000\n",
      "Epoch 110/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 4.0432e-06 - acc: 1.0000 - val_loss: 2.5141e-06 - val_acc: 1.0000\n",
      "Epoch 111/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 4.1340e-06 - acc: 1.0000 - val_loss: 2.4761e-06 - val_acc: 1.0000\n",
      "Epoch 112/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.8823e-06 - acc: 1.0000 - val_loss: 2.4312e-06 - val_acc: 1.0000\n",
      "Epoch 113/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.9156e-06 - acc: 1.0000 - val_loss: 2.3923e-06 - val_acc: 1.0000\n",
      "Epoch 114/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.8513e-06 - acc: 1.0000 - val_loss: 2.3502e-06 - val_acc: 1.0000\n",
      "Epoch 115/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.7376e-06 - acc: 1.0000 - val_loss: 2.3227e-06 - val_acc: 1.0000\n",
      "Epoch 116/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.7898e-06 - acc: 1.0000 - val_loss: 2.2807e-06 - val_acc: 1.0000\n",
      "Epoch 117/474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 2s 53ms/step - loss: 3.6235e-06 - acc: 1.0000 - val_loss: 2.2462e-06 - val_acc: 1.0000\n",
      "Epoch 118/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.4969e-06 - acc: 1.0000 - val_loss: 2.2099e-06 - val_acc: 1.0000\n",
      "Epoch 119/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.5957e-06 - acc: 1.0000 - val_loss: 2.1743e-06 - val_acc: 1.0000\n",
      "Epoch 120/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 3.4491e-06 - acc: 1.0000 - val_loss: 2.1322e-06 - val_acc: 1.0000\n",
      "Epoch 121/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.5234e-06 - acc: 1.0000 - val_loss: 2.0876e-06 - val_acc: 1.0000\n",
      "Epoch 122/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.4341e-06 - acc: 1.0000 - val_loss: 2.0617e-06 - val_acc: 1.0000\n",
      "Epoch 123/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.2188e-06 - acc: 1.0000 - val_loss: 2.0320e-06 - val_acc: 1.0000\n",
      "Epoch 124/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.2259e-06 - acc: 1.0000 - val_loss: 2.0019e-06 - val_acc: 1.0000\n",
      "Epoch 125/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.2442e-06 - acc: 1.0000 - val_loss: 1.9586e-06 - val_acc: 1.0000\n",
      "Epoch 126/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 3.1195e-06 - acc: 1.0000 - val_loss: 1.9306e-06 - val_acc: 1.0000\n",
      "Epoch 127/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.1310e-06 - acc: 1.0000 - val_loss: 1.8993e-06 - val_acc: 1.0000\n",
      "Epoch 128/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.0277e-06 - acc: 1.0000 - val_loss: 1.8765e-06 - val_acc: 1.0000\n",
      "Epoch 129/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 2.9692e-06 - acc: 1.0000 - val_loss: 1.8492e-06 - val_acc: 1.0000\n",
      "Epoch 130/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.8850e-06 - acc: 1.0000 - val_loss: 1.8150e-06 - val_acc: 1.0000\n",
      "Epoch 131/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.8309e-06 - acc: 1.0000 - val_loss: 1.7884e-06 - val_acc: 1.0000\n",
      "Epoch 132/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.8376e-06 - acc: 1.0000 - val_loss: 1.7594e-06 - val_acc: 1.0000\n",
      "Epoch 133/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.7756e-06 - acc: 1.0000 - val_loss: 1.7352e-06 - val_acc: 1.0000\n",
      "Epoch 134/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.7618e-06 - acc: 1.0000 - val_loss: 1.7027e-06 - val_acc: 1.0000\n",
      "Epoch 135/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.6723e-06 - acc: 1.0000 - val_loss: 1.6751e-06 - val_acc: 1.0000\n",
      "Epoch 136/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 2.6024e-06 - acc: 1.0000 - val_loss: 1.6547e-06 - val_acc: 1.0000\n",
      "Epoch 137/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.6374e-06 - acc: 1.0000 - val_loss: 1.6165e-06 - val_acc: 1.0000\n",
      "Epoch 138/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 2.4908e-06 - acc: 1.0000 - val_loss: 1.5861e-06 - val_acc: 1.0000\n",
      "Epoch 139/474\n",
      "34/34 [==============================] - 2s 63ms/step - loss: 2.4444e-06 - acc: 1.0000 - val_loss: 1.5587e-06 - val_acc: 1.0000\n",
      "Epoch 140/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 2.4587e-06 - acc: 1.0000 - val_loss: 1.5348e-06 - val_acc: 1.0000\n",
      "Epoch 141/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.4406e-06 - acc: 1.0000 - val_loss: 1.5177e-06 - val_acc: 1.0000\n",
      "Epoch 142/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 2.3853e-06 - acc: 1.0000 - val_loss: 1.4951e-06 - val_acc: 1.0000\n",
      "Epoch 143/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.2713e-06 - acc: 1.0000 - val_loss: 1.4742e-06 - val_acc: 1.0000\n",
      "Epoch 144/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.2992e-06 - acc: 1.0000 - val_loss: 1.4469e-06 - val_acc: 1.0000\n",
      "Epoch 145/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 2.2970e-06 - acc: 1.0000 - val_loss: 1.4326e-06 - val_acc: 1.0000\n",
      "Epoch 146/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.2180e-06 - acc: 1.0000 - val_loss: 1.4146e-06 - val_acc: 1.0000\n",
      "Epoch 147/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.2179e-06 - acc: 1.0000 - val_loss: 1.3825e-06 - val_acc: 1.0000\n",
      "Epoch 148/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.1369e-06 - acc: 1.0000 - val_loss: 1.3498e-06 - val_acc: 1.0000\n",
      "Epoch 149/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.1362e-06 - acc: 1.0000 - val_loss: 1.3322e-06 - val_acc: 1.0000\n",
      "Epoch 150/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.0684e-06 - acc: 1.0000 - val_loss: 1.3215e-06 - val_acc: 1.0000\n",
      "Epoch 151/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.0488e-06 - acc: 1.0000 - val_loss: 1.2985e-06 - val_acc: 1.0000\n",
      "Epoch 152/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.9960e-06 - acc: 1.0000 - val_loss: 1.2800e-06 - val_acc: 1.0000\n",
      "Epoch 153/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.9229e-06 - acc: 1.0000 - val_loss: 1.2586e-06 - val_acc: 1.0000\n",
      "Epoch 154/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 1.9123e-06 - acc: 1.0000 - val_loss: 1.2427e-06 - val_acc: 1.0000\n",
      "Epoch 155/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.8648e-06 - acc: 1.0000 - val_loss: 1.2208e-06 - val_acc: 1.0000\n",
      "Epoch 156/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 1.8620e-06 - acc: 1.0000 - val_loss: 1.1999e-06 - val_acc: 1.0000\n",
      "Epoch 157/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.8370e-06 - acc: 1.0000 - val_loss: 1.1843e-06 - val_acc: 1.0000\n",
      "Epoch 158/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.8477e-06 - acc: 1.0000 - val_loss: 1.1738e-06 - val_acc: 1.0000\n",
      "Epoch 159/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.7733e-06 - acc: 1.0000 - val_loss: 1.1588e-06 - val_acc: 1.0000\n",
      "Epoch 160/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.7899e-06 - acc: 1.0000 - val_loss: 1.1351e-06 - val_acc: 1.0000\n",
      "Epoch 161/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.7585e-06 - acc: 1.0000 - val_loss: 1.1040e-06 - val_acc: 1.0000\n",
      "Epoch 162/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.6955e-06 - acc: 1.0000 - val_loss: 1.0883e-06 - val_acc: 1.0000\n",
      "Epoch 163/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.6624e-06 - acc: 1.0000 - val_loss: 1.0867e-06 - val_acc: 1.0000\n",
      "Epoch 164/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.6207e-06 - acc: 1.0000 - val_loss: 1.0734e-06 - val_acc: 1.0000\n",
      "Epoch 165/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.6009e-06 - acc: 1.0000 - val_loss: 1.0570e-06 - val_acc: 1.0000\n",
      "Epoch 166/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.5954e-06 - acc: 1.0000 - val_loss: 1.0254e-06 - val_acc: 1.0000\n",
      "Epoch 167/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.5566e-06 - acc: 1.0000 - val_loss: 9.9903e-07 - val_acc: 1.0000\n",
      "Epoch 168/474\n",
      "34/34 [==============================] - 2s 62ms/step - loss: 1.5201e-06 - acc: 1.0000 - val_loss: 9.8930e-07 - val_acc: 1.0000\n",
      "Epoch 169/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.5133e-06 - acc: 1.0000 - val_loss: 9.7291e-07 - val_acc: 1.0000\n",
      "Epoch 170/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.4673e-06 - acc: 1.0000 - val_loss: 9.6507e-07 - val_acc: 1.0000\n",
      "Epoch 171/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.4542e-06 - acc: 1.0000 - val_loss: 9.5795e-07 - val_acc: 1.0000\n",
      "Epoch 172/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.3890e-06 - acc: 1.0000 - val_loss: 9.3919e-07 - val_acc: 1.0000\n",
      "Epoch 173/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.4009e-06 - acc: 1.0000 - val_loss: 9.1687e-07 - val_acc: 1.0000\n",
      "Epoch 174/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 1.3956e-06 - acc: 1.0000 - val_loss: 9.0001e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.3289e-06 - acc: 1.0000 - val_loss: 8.9217e-07 - val_acc: 1.0000\n",
      "Epoch 176/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.3456e-06 - acc: 1.0000 - val_loss: 8.7911e-07 - val_acc: 1.0000\n",
      "Epoch 177/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.2932e-06 - acc: 1.0000 - val_loss: 8.6985e-07 - val_acc: 1.0000\n",
      "Epoch 178/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.2880e-06 - acc: 1.0000 - val_loss: 8.5655e-07 - val_acc: 1.0000\n",
      "Epoch 179/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.2635e-06 - acc: 1.0000 - val_loss: 8.3518e-07 - val_acc: 1.0000\n",
      "Epoch 180/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.2524e-06 - acc: 1.0000 - val_loss: 8.2710e-07 - val_acc: 1.0000\n",
      "Epoch 181/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.1955e-06 - acc: 1.0000 - val_loss: 8.0858e-07 - val_acc: 1.0000\n",
      "Epoch 182/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.1893e-06 - acc: 1.0000 - val_loss: 7.9433e-07 - val_acc: 1.0000\n",
      "Epoch 183/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.2094e-06 - acc: 1.0000 - val_loss: 7.8008e-07 - val_acc: 1.0000\n",
      "Epoch 184/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.2053e-06 - acc: 1.0000 - val_loss: 7.7534e-07 - val_acc: 1.0000\n",
      "Epoch 185/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.1894e-06 - acc: 1.0000 - val_loss: 7.5753e-07 - val_acc: 1.0000\n",
      "Epoch 186/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.1401e-06 - acc: 1.0000 - val_loss: 7.3805e-07 - val_acc: 1.0000\n",
      "Epoch 187/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.1062e-06 - acc: 1.0000 - val_loss: 7.3259e-07 - val_acc: 1.0000\n",
      "Epoch 188/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.0976e-06 - acc: 1.0000 - val_loss: 7.2191e-07 - val_acc: 1.0000\n",
      "Epoch 189/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.0483e-06 - acc: 1.0000 - val_loss: 7.1549e-07 - val_acc: 1.0000\n",
      "Epoch 190/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.0457e-06 - acc: 1.0000 - val_loss: 7.0362e-07 - val_acc: 1.0000\n",
      "Epoch 191/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.0418e-06 - acc: 1.0000 - val_loss: 6.7869e-07 - val_acc: 1.0000\n",
      "Epoch 192/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 9.9499e-07 - acc: 1.0000 - val_loss: 6.6563e-07 - val_acc: 1.0000\n",
      "Epoch 193/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 1.0018e-06 - acc: 1.0000 - val_loss: 6.5565e-07 - val_acc: 1.0000\n",
      "Epoch 194/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 9.6703e-07 - acc: 1.0000 - val_loss: 6.4995e-07 - val_acc: 1.0000\n",
      "Epoch 195/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 9.5460e-07 - acc: 1.0000 - val_loss: 6.3950e-07 - val_acc: 1.0000\n",
      "Epoch 196/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 9.3252e-07 - acc: 1.0000 - val_loss: 6.3618e-07 - val_acc: 1.0000\n",
      "Epoch 197/474\n",
      "34/34 [==============================] - 2s 59ms/step - loss: 9.3502e-07 - acc: 1.0000 - val_loss: 6.2169e-07 - val_acc: 1.0000\n",
      "Epoch 198/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 9.5093e-07 - acc: 1.0000 - val_loss: 6.0816e-07 - val_acc: 1.0000\n",
      "Epoch 199/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 8.9609e-07 - acc: 1.0000 - val_loss: 6.0175e-07 - val_acc: 1.0000\n",
      "Epoch 200/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 8.7633e-07 - acc: 1.0000 - val_loss: 5.8702e-07 - val_acc: 1.0000\n",
      "Epoch 201/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 8.8325e-07 - acc: 1.0000 - val_loss: 5.8512e-07 - val_acc: 1.0000\n",
      "Epoch 202/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 8.3941e-07 - acc: 1.0000 - val_loss: 5.7800e-07 - val_acc: 1.0000\n",
      "Epoch 203/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 8.3771e-07 - acc: 1.0000 - val_loss: 5.7182e-07 - val_acc: 1.0000\n",
      "Epoch 204/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 8.0990e-07 - acc: 1.0000 - val_loss: 5.6185e-07 - val_acc: 1.0000\n",
      "Epoch 205/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 8.0951e-07 - acc: 1.0000 - val_loss: 5.3858e-07 - val_acc: 1.0000\n",
      "Epoch 206/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 8.0100e-07 - acc: 1.0000 - val_loss: 5.1531e-07 - val_acc: 1.0000\n",
      "Epoch 207/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 7.8959e-07 - acc: 1.0000 - val_loss: 5.0367e-07 - val_acc: 1.0000\n",
      "Epoch 208/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 7.4068e-07 - acc: 1.0000 - val_loss: 5.0367e-07 - val_acc: 1.0000\n",
      "Epoch 209/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 7.5959e-07 - acc: 1.0000 - val_loss: 4.9465e-07 - val_acc: 1.0000\n",
      "Epoch 210/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 7.2246e-07 - acc: 1.0000 - val_loss: 4.8752e-07 - val_acc: 1.0000\n",
      "Epoch 211/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 7.2377e-07 - acc: 1.0000 - val_loss: 4.8159e-07 - val_acc: 1.0000\n",
      "Epoch 212/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 7.0872e-07 - acc: 1.0000 - val_loss: 4.7802e-07 - val_acc: 1.0000\n",
      "Epoch 213/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 6.9651e-07 - acc: 1.0000 - val_loss: 4.7019e-07 - val_acc: 1.0000\n",
      "Epoch 214/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 6.7227e-07 - acc: 1.0000 - val_loss: 4.6330e-07 - val_acc: 1.0000\n",
      "Epoch 215/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 6.6257e-07 - acc: 1.0000 - val_loss: 4.5974e-07 - val_acc: 1.0000\n",
      "Epoch 216/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 6.5644e-07 - acc: 1.0000 - val_loss: 4.4834e-07 - val_acc: 1.0000\n",
      "Epoch 217/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 6.5280e-07 - acc: 1.0000 - val_loss: 4.5190e-07 - val_acc: 1.0000\n",
      "Epoch 218/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 6.4545e-07 - acc: 1.0000 - val_loss: 4.3647e-07 - val_acc: 1.0000\n",
      "Epoch 219/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 6.1459e-07 - acc: 1.0000 - val_loss: 4.2364e-07 - val_acc: 1.0000\n",
      "Epoch 220/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 6.2625e-07 - acc: 1.0000 - val_loss: 4.1462e-07 - val_acc: 1.0000\n",
      "Epoch 221/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 6.0964e-07 - acc: 1.0000 - val_loss: 3.9064e-07 - val_acc: 1.0000\n",
      "Epoch 222/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 5.9675e-07 - acc: 1.0000 - val_loss: 3.7971e-07 - val_acc: 1.0000\n",
      "Epoch 223/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 5.8356e-07 - acc: 1.0000 - val_loss: 3.7734e-07 - val_acc: 1.0000\n",
      "Epoch 224/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 5.9426e-07 - acc: 1.0000 - val_loss: 3.7520e-07 - val_acc: 1.0000\n",
      "Epoch 225/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 5.8310e-07 - acc: 1.0000 - val_loss: 3.6784e-07 - val_acc: 1.0000\n",
      "Epoch 226/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 5.6013e-07 - acc: 1.0000 - val_loss: 3.6641e-07 - val_acc: 1.0000\n",
      "Epoch 227/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 5.1921e-07 - acc: 1.0000 - val_loss: 3.6238e-07 - val_acc: 1.0000\n",
      "Epoch 228/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 5.2504e-07 - acc: 1.0000 - val_loss: 3.5905e-07 - val_acc: 1.0000\n",
      "Epoch 229/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 5.1774e-07 - acc: 1.0000 - val_loss: 3.5597e-07 - val_acc: 1.0000\n",
      "Epoch 230/474\n",
      "34/34 [==============================] - 2s 63ms/step - loss: 5.1122e-07 - acc: 1.0000 - val_loss: 3.5074e-07 - val_acc: 1.0000\n",
      "Epoch 231/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 5.1040e-07 - acc: 1.0000 - val_loss: 3.4647e-07 - val_acc: 1.0000\n",
      "Epoch 232/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 4.8995e-07 - acc: 1.0000 - val_loss: 3.3816e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 4.7563e-07 - acc: 1.0000 - val_loss: 3.4101e-07 - val_acc: 1.0000\n",
      "Epoch 234/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 4.9221e-07 - acc: 1.0000 - val_loss: 3.1037e-07 - val_acc: 1.0000\n",
      "Epoch 235/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 4.6666e-07 - acc: 1.0000 - val_loss: 2.8971e-07 - val_acc: 1.0000\n",
      "Epoch 236/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 4.5462e-07 - acc: 1.0000 - val_loss: 2.8781e-07 - val_acc: 1.0000\n",
      "Epoch 237/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 4.2936e-07 - acc: 1.0000 - val_loss: 2.8021e-07 - val_acc: 1.0000\n",
      "Epoch 238/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 4.4473e-07 - acc: 1.0000 - val_loss: 2.7974e-07 - val_acc: 1.0000\n",
      "Epoch 239/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 4.2787e-07 - acc: 1.0000 - val_loss: 2.7855e-07 - val_acc: 1.0000\n",
      "Epoch 240/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 4.1773e-07 - acc: 1.0000 - val_loss: 2.6478e-07 - val_acc: 1.0000\n",
      "Epoch 241/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 4.0880e-07 - acc: 1.0000 - val_loss: 2.6169e-07 - val_acc: 1.0000\n",
      "Epoch 242/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 4.2079e-07 - acc: 1.0000 - val_loss: 2.6027e-07 - val_acc: 1.0000\n",
      "Epoch 243/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.9052e-07 - acc: 1.0000 - val_loss: 2.5694e-07 - val_acc: 1.0000\n",
      "Epoch 244/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.8232e-07 - acc: 1.0000 - val_loss: 2.5053e-07 - val_acc: 1.0000\n",
      "Epoch 245/474\n",
      "34/34 [==============================] - 2s 60ms/step - loss: 3.8082e-07 - acc: 1.0000 - val_loss: 2.4982e-07 - val_acc: 1.0000\n",
      "Epoch 246/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 3.8108e-07 - acc: 1.0000 - val_loss: 2.4792e-07 - val_acc: 1.0000\n",
      "Epoch 247/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.6421e-07 - acc: 1.0000 - val_loss: 2.4626e-07 - val_acc: 1.0000\n",
      "Epoch 248/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 3.4976e-07 - acc: 1.0000 - val_loss: 2.4174e-07 - val_acc: 1.0000\n",
      "Epoch 249/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 3.4497e-07 - acc: 1.0000 - val_loss: 2.3533e-07 - val_acc: 1.0000\n",
      "Epoch 250/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 3.3520e-07 - acc: 1.0000 - val_loss: 2.3224e-07 - val_acc: 1.0000\n",
      "Epoch 251/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.4571e-07 - acc: 1.0000 - val_loss: 2.2702e-07 - val_acc: 1.0000\n",
      "Epoch 252/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 3.3076e-07 - acc: 1.0000 - val_loss: 2.3343e-07 - val_acc: 1.0000\n",
      "Epoch 253/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.1398e-07 - acc: 1.0000 - val_loss: 2.2275e-07 - val_acc: 1.0000\n",
      "Epoch 254/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 3.1629e-07 - acc: 1.0000 - val_loss: 2.0850e-07 - val_acc: 1.0000\n",
      "Epoch 255/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.1987e-07 - acc: 1.0000 - val_loss: 2.1040e-07 - val_acc: 1.0000\n",
      "Epoch 256/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 3.1317e-07 - acc: 1.0000 - val_loss: 1.8594e-07 - val_acc: 1.0000\n",
      "Epoch 257/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 3.0159e-07 - acc: 1.0000 - val_loss: 1.7288e-07 - val_acc: 1.0000\n",
      "Epoch 258/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.8129e-07 - acc: 1.0000 - val_loss: 1.7027e-07 - val_acc: 1.0000\n",
      "Epoch 259/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.7616e-07 - acc: 1.0000 - val_loss: 1.6314e-07 - val_acc: 1.0000\n",
      "Epoch 260/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.7429e-07 - acc: 1.0000 - val_loss: 1.5507e-07 - val_acc: 1.0000\n",
      "Epoch 261/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.5770e-07 - acc: 1.0000 - val_loss: 1.5151e-07 - val_acc: 1.0000\n",
      "Epoch 262/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.5322e-07 - acc: 1.0000 - val_loss: 1.4889e-07 - val_acc: 1.0000\n",
      "Epoch 263/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 2.5046e-07 - acc: 1.0000 - val_loss: 1.4984e-07 - val_acc: 1.0000\n",
      "Epoch 264/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.4893e-07 - acc: 1.0000 - val_loss: 1.4224e-07 - val_acc: 1.0000\n",
      "Epoch 265/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.4228e-07 - acc: 1.0000 - val_loss: 1.4438e-07 - val_acc: 1.0000\n",
      "Epoch 266/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.3993e-07 - acc: 1.0000 - val_loss: 1.3536e-07 - val_acc: 1.0000\n",
      "Epoch 267/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 2.2651e-07 - acc: 1.0000 - val_loss: 1.3203e-07 - val_acc: 1.0000\n",
      "Epoch 268/474\n",
      "34/34 [==============================] - 2s 71ms/step - loss: 2.2620e-07 - acc: 1.0000 - val_loss: 1.3631e-07 - val_acc: 1.0000\n",
      "Epoch 269/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.1915e-07 - acc: 1.0000 - val_loss: 1.3108e-07 - val_acc: 1.0000\n",
      "Epoch 270/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.1160e-07 - acc: 1.0000 - val_loss: 1.3013e-07 - val_acc: 1.0000\n",
      "Epoch 271/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.1217e-07 - acc: 1.0000 - val_loss: 1.2752e-07 - val_acc: 1.0000\n",
      "Epoch 272/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.0196e-07 - acc: 1.0000 - val_loss: 1.2372e-07 - val_acc: 1.0000\n",
      "Epoch 273/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.9720e-07 - acc: 1.0000 - val_loss: 1.2063e-07 - val_acc: 1.0000\n",
      "Epoch 274/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.9404e-07 - acc: 1.0000 - val_loss: 1.2040e-07 - val_acc: 1.0000\n",
      "Epoch 275/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 1.9053e-07 - acc: 1.0000 - val_loss: 1.1897e-07 - val_acc: 1.0000\n",
      "Epoch 276/474\n",
      "34/34 [==============================] - 2s 63ms/step - loss: 1.8170e-07 - acc: 1.0000 - val_loss: 1.1826e-07 - val_acc: 1.0000\n",
      "Epoch 277/474\n",
      "34/34 [==============================] - 2s 64ms/step - loss: 1.8307e-07 - acc: 1.0000 - val_loss: 1.1446e-07 - val_acc: 1.0000\n",
      "Epoch 278/474\n",
      "34/34 [==============================] - 2s 64ms/step - loss: 1.6676e-07 - acc: 1.0000 - val_loss: 1.1256e-07 - val_acc: 1.0000\n",
      "Epoch 279/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 1.7192e-07 - acc: 1.0000 - val_loss: 1.0924e-07 - val_acc: 1.0000\n",
      "Epoch 280/474\n",
      "34/34 [==============================] - 3s 75ms/step - loss: 1.6775e-07 - acc: 1.0000 - val_loss: 1.0544e-07 - val_acc: 1.0000\n",
      "Epoch 281/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 1.6592e-07 - acc: 1.0000 - val_loss: 1.0520e-07 - val_acc: 1.0000\n",
      "Epoch 282/474\n",
      "34/34 [==============================] - 2s 60ms/step - loss: 1.6457e-07 - acc: 1.0000 - val_loss: 1.0686e-07 - val_acc: 1.0000\n",
      "Epoch 283/474\n",
      "34/34 [==============================] - 2s 62ms/step - loss: 1.6034e-07 - acc: 1.0000 - val_loss: 1.0187e-07 - val_acc: 1.0000\n",
      "Epoch 284/474\n",
      "34/34 [==============================] - 2s 66ms/step - loss: 1.5936e-07 - acc: 1.0000 - val_loss: 9.9974e-08 - val_acc: 1.0000\n",
      "Epoch 285/474\n",
      "34/34 [==============================] - 2s 59ms/step - loss: 1.4898e-07 - acc: 1.0000 - val_loss: 9.4750e-08 - val_acc: 1.0000\n",
      "Epoch 286/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 1.4388e-07 - acc: 1.0000 - val_loss: 9.2613e-08 - val_acc: 1.0000\n",
      "Epoch 287/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 1.4193e-07 - acc: 1.0000 - val_loss: 9.3325e-08 - val_acc: 1.0000\n",
      "Epoch 288/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 1.3985e-07 - acc: 1.0000 - val_loss: 8.5726e-08 - val_acc: 1.0000\n",
      "Epoch 289/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 1.4307e-07 - acc: 1.0000 - val_loss: 8.3826e-08 - val_acc: 1.0000\n",
      "Epoch 290/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.3680e-07 - acc: 1.0000 - val_loss: 8.3589e-08 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 1.3501e-07 - acc: 1.0000 - val_loss: 7.9315e-08 - val_acc: 1.0000\n",
      "Epoch 292/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 1.2913e-07 - acc: 1.0000 - val_loss: 7.4565e-08 - val_acc: 1.0000\n",
      "Epoch 293/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 1.2974e-07 - acc: 1.0000 - val_loss: 7.1716e-08 - val_acc: 1.0000\n",
      "Epoch 294/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 1.1905e-07 - acc: 1.0000 - val_loss: 4.6544e-08 - val_acc: 1.0000\n",
      "Epoch 295/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.1906e-07 - acc: 1.0000 - val_loss: 3.9182e-08 - val_acc: 1.0000\n",
      "Epoch 296/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 1.1470e-07 - acc: 1.0000 - val_loss: 3.6570e-08 - val_acc: 1.0000\n",
      "Epoch 297/474\n",
      "34/34 [==============================] - 2s 59ms/step - loss: 1.0491e-07 - acc: 1.0000 - val_loss: 2.3509e-08 - val_acc: 1.0000\n",
      "Epoch 298/474\n",
      "34/34 [==============================] - 2s 59ms/step - loss: 1.0231e-07 - acc: 1.0000 - val_loss: 1.8285e-08 - val_acc: 1.0000\n",
      "Epoch 299/474\n",
      "34/34 [==============================] - 2s 58ms/step - loss: 9.5959e-08 - acc: 1.0000 - val_loss: 1.5910e-08 - val_acc: 1.0000\n",
      "Epoch 300/474\n",
      "34/34 [==============================] - 2s 72ms/step - loss: 8.6793e-08 - acc: 1.0000 - val_loss: 1.3536e-08 - val_acc: 1.0000\n",
      "Epoch 301/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 8.3203e-08 - acc: 1.0000 - val_loss: 1.5435e-08 - val_acc: 1.0000\n",
      "Epoch 302/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 7.5778e-08 - acc: 1.0000 - val_loss: 1.3536e-08 - val_acc: 1.0000\n",
      "Epoch 303/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 7.5276e-08 - acc: 1.0000 - val_loss: 1.2586e-08 - val_acc: 1.0000\n",
      "Epoch 304/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 7.0661e-08 - acc: 1.0000 - val_loss: 8.3114e-09 - val_acc: 1.0000\n",
      "Epoch 305/474\n",
      "34/34 [==============================] - 2s 59ms/step - loss: 6.4601e-08 - acc: 1.0000 - val_loss: 1.8997e-09 - val_acc: 1.0000\n",
      "Epoch 306/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 5.9534e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 307/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 5.5961e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 308/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 5.2234e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 309/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 4.6713e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 310/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 4.7401e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 311/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 4.7356e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 312/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 4.6706e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 313/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 4.2005e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 314/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 4.2478e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 315/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.9179e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 316/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.5611e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 317/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.5060e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 318/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 3.3751e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 319/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 3.1313e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 320/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.2006e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 321/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 2.6791e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 322/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.2973e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 323/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 2.9518e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 324/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.5094e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 325/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 2.8332e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 326/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 2.4736e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 327/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.5901e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 328/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.2906e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 329/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 2.6783e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 330/474\n",
      "34/34 [==============================] - 2s 64ms/step - loss: 2.3431e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 331/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 1.9186e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 332/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 2.1065e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 333/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 2.2724e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 334/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 2.5242e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 335/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 1.9664e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 336/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.8263e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 337/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.9958e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 338/474\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 1.8148e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 339/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 1.7766e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 340/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 1.8738e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 341/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.7859e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 342/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 1.8709e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 343/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 1.7025e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 344/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 1.9108e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 345/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.6875e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 346/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.5775e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 347/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 1.6228e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 348/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 1.7141e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 1.9799e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 350/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.4870e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 351/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 1.9231e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 352/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.5760e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 353/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.2993e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 354/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 1.2995e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 355/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 1.5710e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 356/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 1.4379e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 357/474\n",
      "34/34 [==============================] - 2s 63ms/step - loss: 8.9648e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 358/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 1.0946e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 359/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 1.3384e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 360/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.1882e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 361/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 1.0701e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 362/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 1.3359e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 363/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 1.2472e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 364/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.0172e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 365/474\n",
      "34/34 [==============================] - 2s 44ms/step - loss: 1.2332e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 366/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.1041e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 367/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 1.0616e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 368/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 8.9660e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 369/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 1.0553e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 370/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.0257e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 371/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 9.3356e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 372/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 1.0633e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 373/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 8.1899e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 374/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 9.8728e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 375/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 9.0613e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 376/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 9.4202e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 377/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 9.1606e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 378/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 8.9682e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 379/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 1.0505e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 380/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 8.6681e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 381/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 6.9519e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 382/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 8.1440e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 383/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 9.2849e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 384/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 7.8162e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 385/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 7.7996e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 386/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 7.3138e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 387/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 7.4560e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 388/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 9.1550e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 389/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 7.5322e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 390/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 6.0742e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 391/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 1.0879e-08 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 392/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 7.6609e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 393/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 7.1248e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 394/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 7.5236e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 395/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 5.0916e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 396/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 5.4006e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 397/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 6.1152e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 398/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 6.9912e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 399/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 4.7572e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 400/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 8.2669e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 401/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 5.2332e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 402/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 6.6929e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 403/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 6.4449e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 404/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 7.2804e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 405/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 4.5799e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 406/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 4.2922e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/474\n",
      "34/34 [==============================] - 2s 47ms/step - loss: 4.8710e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 408/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 3.2479e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 409/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 6.9433e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 410/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 4.6508e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 411/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 5.5792e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 412/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 5.0957e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 413/474\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 5.4109e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 414/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 7.2629e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 415/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 6.1854e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 416/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 5.0572e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 417/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 4.9727e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 418/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 4.8651e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 419/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 4.4518e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 420/474\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 5.7318e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 421/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 4.3477e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 422/474\n",
      "34/34 [==============================] - 1s 41ms/step - loss: 4.9760e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 423/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 3.0254e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 424/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 3.8030e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 425/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 4.4476e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 426/474\n",
      "34/34 [==============================] - 1s 42ms/step - loss: 4.5519e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 427/474\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 3.2976e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 428/474\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 3.6299e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 429/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 5.4462e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 430/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 2.9446e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 431/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 4.9060e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 432/474\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 3.7564e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 433/474\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 4.0876e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 434/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 3.2585e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 435/474\n",
      "34/34 [==============================] - 1s 41ms/step - loss: 4.2577e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 436/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 3.6568e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 437/474\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 2.3346e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 438/474\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 3.2007e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 439/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 2.3317e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 440/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 4.5495e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 441/474\n",
      "34/34 [==============================] - 1s 44ms/step - loss: 3.2011e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 442/474\n",
      "34/34 [==============================] - 1s 43ms/step - loss: 2.2621e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 443/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 3.0541e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 444/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 2.4213e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 445/474\n",
      "34/34 [==============================] - 2s 45ms/step - loss: 2.5512e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 446/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 2.0729e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 447/474\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 1.3808e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 448/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.5726e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 449/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.5312e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 450/474\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 3.0874e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 451/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.6646e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 452/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 3.2588e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 453/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.0136e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 454/474\n",
      "34/34 [==============================] - 2s 58ms/step - loss: 2.5538e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 455/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 2.9754e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 456/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 2.3304e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 457/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 2.6671e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 458/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 2.5466e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 459/474\n",
      "34/34 [==============================] - 2s 48ms/step - loss: 2.3284e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 460/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 1.7183e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 461/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.9613e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 462/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 2.0930e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 463/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 2.8219e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 464/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.4188e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/474\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 1.7688e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 466/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 1.8537e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 467/474\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 2.7570e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 468/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.7683e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 469/474\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 3.1039e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 470/474\n",
      "34/34 [==============================] - 2s 60ms/step - loss: 1.5430e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 471/474\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 1.9164e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 472/474\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 1.4912e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 473/474\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 1.9002e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 474/474\n",
      "34/34 [==============================] - 2s 46ms/step - loss: 1.4797e-09 - acc: 1.0000 - val_loss: 0.0000e+00 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# epochs = 500\n",
    "# batch_size = 512\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(90, dropout=0.2, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "# model.add(Dense(100, activation='sigmoid'))\n",
    "# model.add(Dense(40, activation='sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "## RESULT OPTIMIZATION 1\n",
    "# epochs = 330\n",
    "# batch_size = 128\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(26, dropout=0.6, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "# model.add(LSTM(6, dropout=0.38, return_sequences=True))\n",
    "# model.add(Dense(95, activation='sigmoid'))\n",
    "# model.add(Dense(74, activation='sigmoid'))\n",
    "# model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "## RESULT OPTIMIZATION 2\n",
    "# epochs = 95\n",
    "# batch_size = 256\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(66, dropout=0.41, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "# model.add(LSTM(128, dropout=0.31, return_sequences=True))\n",
    "# model.add(Dense(130, activation='sigmoid'))\n",
    "# model.add(Dense(28, activation='sigmoid'))\n",
    "# model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# RESULT OPTIMIZATION 3\n",
    "# tensorflow.random.set_seed(111)\n",
    "# np.random.seed(111)\n",
    "# random.seed(111)\n",
    "\n",
    "# epochs = 491\n",
    "# batch_size = 64\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(109, dropout=0.28, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "# model.add(LSTM(329, dropout=0.16, return_sequences=True))\n",
    "# model.add(Dense(66, activation='tanh'))\n",
    "# model.add(Dense(158, activation='tanh'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "tensorflow.random.set_seed(111)\n",
    "np.random.seed(111)\n",
    "random.seed(111)\n",
    "\n",
    "epochs = 474\n",
    "batch_size = 64\n",
    "model = Sequential()\n",
    "model.add(LSTM(322, dropout=0.89, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(61, dropout=0.79, return_sequences=True))\n",
    "model.add(Dense(262, activation='sigmoid'))\n",
    "model.add(Dense(282, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"acc\"])\n",
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(x_val, y_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Train Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473
         ],
         "xaxis": "x",
         "y": [
          0.057695142924785614,
          0.0003822548605967313,
          0.00032691285014152527,
          0.0003015127149410546,
          0.00027723461971618235,
          0.0002535640087444335,
          0.0002315105957677588,
          0.00021212419960647821,
          0.0001930818980326876,
          0.0001766076748026535,
          0.0001615491055417806,
          0.00014708456001244485,
          0.0001355001440970227,
          0.00012400068226270378,
          0.00011357419134583324,
          0.00010452017158968374,
          9.55978175625205e-05,
          8.78597711562179e-05,
          8.184422040358186e-05,
          7.48025777284056e-05,
          7.023165380815044e-05,
          6.4332147303503e-05,
          5.975198291707784e-05,
          5.515123120858334e-05,
          5.1988234190503135e-05,
          4.8684330977266654e-05,
          4.555296982289292e-05,
          4.294889367884025e-05,
          4.013630677945912e-05,
          3.837146869045682e-05,
          3.637270856415853e-05,
          3.38662757712882e-05,
          3.2013049349188805e-05,
          3.0404768040170893e-05,
          2.9779406759189442e-05,
          2.7630758268060163e-05,
          2.6650706786313094e-05,
          2.547764415794518e-05,
          2.455234971421305e-05,
          2.359573045396246e-05,
          2.2562775484402664e-05,
          2.1785786884720437e-05,
          2.0949031750205904e-05,
          2.0239986042724922e-05,
          1.9259457985754125e-05,
          1.8840195480152033e-05,
          1.8262615412822925e-05,
          1.7358013792545535e-05,
          1.7065172869479284e-05,
          1.6252944988082163e-05,
          1.5548554074484855e-05,
          1.5780085959704593e-05,
          1.5134233763092197e-05,
          1.4454908523475751e-05,
          1.3959614989289548e-05,
          1.35086202135426e-05,
          1.3258478247735184e-05,
          1.2937402971147094e-05,
          1.240316305484157e-05,
          1.195839558931766e-05,
          1.163706838269718e-05,
          1.1232754332013428e-05,
          1.1318325050524436e-05,
          1.0874812687688973e-05,
          1.0652037417457905e-05,
          1.0275733984599356e-05,
          9.988695637730416e-06,
          9.740907444211189e-06,
          9.468029929848853e-06,
          9.39211076911306e-06,
          9.143285751633812e-06,
          8.871752470440697e-06,
          8.661003448651172e-06,
          8.712141607247759e-06,
          8.143058948917314e-06,
          7.962336894706823e-06,
          7.876883501012344e-06,
          7.679702321183868e-06,
          7.776233360345941e-06,
          7.498924333049217e-06,
          7.369033482973464e-06,
          7.188143626990495e-06,
          7.029737844277406e-06,
          6.932311862328788e-06,
          6.535428383358521e-06,
          6.413274149963399e-06,
          6.550652869918849e-06,
          6.237206434889231e-06,
          6.2084700402920134e-06,
          6.027047675161157e-06,
          5.941311428614426e-06,
          5.683013114321511e-06,
          5.657949714077404e-06,
          5.532402155949967e-06,
          5.341755695553729e-06,
          5.241218332230346e-06,
          5.288010925141862e-06,
          5.131568286742549e-06,
          5.116598003951367e-06,
          4.94994947075611e-06,
          4.780610652233008e-06,
          4.691931735578692e-06,
          4.637262009055121e-06,
          4.613402779796161e-06,
          4.476082267501624e-06,
          4.439943040779326e-06,
          4.414234808791662e-06,
          4.251287464285269e-06,
          4.194990651740227e-06,
          4.008969881397206e-06,
          4.08135974794277e-06,
          3.880899839714402e-06,
          3.908769031113479e-06,
          3.791408971665078e-06,
          3.68590849575412e-06,
          3.65086361853173e-06,
          3.6002579690830316e-06,
          3.472749313004897e-06,
          3.4938607313961256e-06,
          3.412640126043698e-06,
          3.3846881706267595e-06,
          3.322280008433154e-06,
          3.2101349916047184e-06,
          3.1489882985624718e-06,
          3.1838371796766296e-06,
          3.088934818151756e-06,
          3.096028422078234e-06,
          2.9364180136326468e-06,
          2.9319883196876617e-06,
          2.8266842946322868e-06,
          2.823152044584276e-06,
          2.813704213622259e-06,
          2.727605078689521e-06,
          2.6938489554595435e-06,
          2.6394031920062844e-06,
          2.5738261228980264e-06,
          2.6086756861332105e-06,
          2.46824174610083e-06,
          2.452682338116574e-06,
          2.441439619360608e-06,
          2.4063938326435164e-06,
          2.3799000246071955e-06,
          2.2528124645759817e-06,
          2.273026439070236e-06,
          2.188609414588427e-06,
          2.1674702566087944e-06,
          2.1667412966053234e-06,
          2.116080167979817e-06,
          2.101052587022423e-06,
          2.0422889974724967e-06,
          2.004776661124197e-06,
          1.9923845684388652e-06,
          1.921677949212608e-06,
          1.905388785417017e-06,
          1.8537743926572148e-06,
          1.8256823750562035e-06,
          1.7928234683495248e-06,
          1.8077106460623327e-06,
          1.7386015542797395e-06,
          1.7819454569689697e-06,
          1.7258454363400233e-06,
          1.6835945189086488e-06,
          1.6437832073279424e-06,
          1.6213265325859538e-06,
          1.603327064003679e-06,
          1.561160843266407e-06,
          1.524097001492919e-06,
          1.5019205648059142e-06,
          1.4777533579035662e-06,
          1.4941265362722334e-06,
          1.4159899137666798e-06,
          1.3910096186009469e-06,
          1.3848697335561155e-06,
          1.3578148809756385e-06,
          1.3179754887460149e-06,
          1.3320218386070337e-06,
          1.2815287391276797e-06,
          1.2719124242721591e-06,
          1.2411288707880885e-06,
          1.2394185660014045e-06,
          1.1852806665046955e-06,
          1.1904675147889066e-06,
          1.1794212468885235e-06,
          1.171290932688862e-06,
          1.1655995422188425e-06,
          1.1119946066173725e-06,
          1.0914158110608696e-06,
          1.0768371794256382e-06,
          1.0519690931687364e-06,
          1.0221106094832066e-06,
          1.0324561117158737e-06,
          9.890280807667295e-07,
          9.860002592176897e-07,
          9.612443818696192e-07,
          9.409743029209494e-07,
          9.268160852116125e-07,
          9.077516551769804e-07,
          9.233676792064216e-07,
          8.852665587255615e-07,
          8.650526410747261e-07,
          8.640432724860148e-07,
          8.395118129556067e-07,
          8.212041961996874e-07,
          7.988033985384391e-07,
          8.048871791288548e-07,
          7.92270952842955e-07,
          7.907290751063556e-07,
          7.395631769213651e-07,
          7.506935162382433e-07,
          7.263021188919083e-07,
          7.179193062256672e-07,
          7.121998919501493e-07,
          6.92630806042871e-07,
          6.724447985106963e-07,
          6.65575953462394e-07,
          6.609780598409998e-07,
          6.446330758080876e-07,
          6.348484475893201e-07,
          6.166530397422321e-07,
          6.088870918574685e-07,
          6.104010026319884e-07,
          5.83318126246013e-07,
          5.808510081806162e-07,
          5.795333208880038e-07,
          5.627117047879437e-07,
          5.482731353367853e-07,
          5.265731601866719e-07,
          5.239377856014471e-07,
          5.139849577062705e-07,
          5.058825536252698e-07,
          5.000510441277584e-07,
          4.861732350036618e-07,
          4.7103375777624024e-07,
          4.7582790330125135e-07,
          4.6769744699304283e-07,
          4.4728719217346224e-07,
          4.313066312988667e-07,
          4.42885550455685e-07,
          4.2096132801816566e-07,
          4.1972774056375783e-07,
          4.070834904723597e-07,
          4.1243836790272326e-07,
          3.841219609057589e-07,
          3.781502755373367e-07,
          3.7697276411563507e-07,
          3.673844446439034e-07,
          3.576278970740532e-07,
          3.549364464561222e-07,
          3.421519920721039e-07,
          3.342458398947201e-07,
          3.415912885884609e-07,
          3.228351488360204e-07,
          3.1086375429367763e-07,
          3.14312188720578e-07,
          3.092376630320359e-07,
          3.034342057617323e-07,
          2.9709806881328404e-07,
          2.781456771572266e-07,
          2.7741674557546503e-07,
          2.749495706666494e-07,
          2.625015724788682e-07,
          2.5442719220336585e-07,
          2.4881998683667916e-07,
          2.4559582811889413e-07,
          2.3827843165236118e-07,
          2.326151502529683e-07,
          2.257182671883129e-07,
          2.1955032991627377e-07,
          2.181765665909552e-07,
          2.0741072148666717e-07,
          2.085882329083688e-07,
          1.9381323568268272e-07,
          1.9389733552088728e-07,
          1.8868261975057976e-07,
          1.846454296128286e-07,
          1.8226236875307222e-07,
          1.7861769663340965e-07,
          1.7043115008164023e-07,
          1.691975626272324e-07,
          1.6605753216936137e-07,
          1.6064657870629162e-07,
          1.5851583157200366e-07,
          1.6000174696273461e-07,
          1.567776024558043e-07,
          1.4948822979476972e-07,
          1.4281565086093906e-07,
          1.4318011665181984e-07,
          1.4090919364662113e-07,
          1.3757289707427844e-07,
          1.3569449208716833e-07,
          1.3087228012409469e-07,
          1.2619027245364123e-07,
          1.2450810515929334e-07,
          1.1870464078356235e-07,
          1.1256474863330368e-07,
          1.0959292495726913e-07,
          1.0303249098342349e-07,
          9.924762167656809e-08,
          9.3948806068056e-08,
          8.579031884892174e-08,
          8.004292340046959e-08,
          7.533287060823568e-08,
          7.423946613016597e-08,
          6.809956687447993e-08,
          6.319326217862908e-08,
          5.828695037735088e-08,
          5.4277794703239124e-08,
          5.105365019630881e-08,
          4.735289138579901e-08,
          4.67360976585951e-08,
          4.527822383693092e-08,
          4.429696076613254e-08,
          3.857760688674716e-08,
          3.897011069398104e-08,
          3.6811336201481026e-08,
          3.616650445792402e-08,
          3.3671298638182634e-08,
          3.0811619922133104e-08,
          2.733515103159334e-08,
          3.033500561855362e-08,
          2.7447294215221518e-08,
          3.053126107488424e-08,
          2.898927675687446e-08,
          2.8288376086038625e-08,
          2.859677294964058e-08,
          2.3718499164715467e-08,
          2.6494069160776235e-08,
          2.2569020430296405e-08,
          2.5232447242728995e-08,
          2.545673360998535e-08,
          2.1447579712230436e-08,
          2.074667726503776e-08,
          2.1251326032256657e-08,
          2.254098596665699e-08,
          2.0970965408650954e-08,
          1.8924334099779117e-08,
          1.9849524690584985e-08,
          1.7634675941735622e-08,
          1.8363612852567712e-08,
          1.7915036565341325e-08,
          1.769074842172813e-08,
          1.8055219541679435e-08,
          1.6793594070918516e-08,
          1.735431531812992e-08,
          1.648519720731656e-08,
          1.5195539049273066e-08,
          1.5812334552833818e-08,
          1.6989847750892295e-08,
          1.5475899672878768e-08,
          1.4354458066634379e-08,
          1.802718152532634e-08,
          1.488714307384953e-08,
          1.216764644595969e-08,
          1.2840511764977691e-08,
          1.4466603914797815e-08,
          1.3569448675809781e-08,
          9.588330129872702e-09,
          1.056959142431424e-08,
          1.2868547116795526e-08,
          1.183121334236148e-08,
          1.1494781126941689e-08,
          1.2868547116795526e-08,
          1.1943357414168077e-08,
          1.0205122968898195e-08,
          1.1242456743332241e-08,
          1.1242456743332241e-08,
          1.0008870177102835e-08,
          8.999572642665044e-09,
          9.420113578073597e-09,
          8.635104187248999e-09,
          9.251897026274492e-09,
          1.0962096119726539e-08,
          8.410815155457385e-09,
          8.859393219040612e-09,
          7.934202628234743e-09,
          8.831356090865938e-09,
          8.999572642665044e-09,
          8.77528361087343e-09,
          9.560293889876448e-09,
          8.186527900022611e-09,
          7.261337309216742e-09,
          6.9249046497077416e-09,
          7.990274220048832e-09,
          8.298671083650788e-09,
          7.541697932822444e-09,
          6.420255882488846e-09,
          6.924905093796951e-09,
          7.794022316431892e-09,
          7.149193237410145e-09,
          7.093120757417637e-09,
          8.24259860365828e-09,
          6.98097668561104e-09,
          7.093120757417637e-09,
          6.728652302001592e-09,
          4.682020549040544e-09,
          5.71935432347459e-09,
          4.766128824940097e-09,
          6.476327918392144e-09,
          4.513804441330649e-09,
          6.5323999542954425e-09,
          5.607210251667993e-09,
          6.111859018886889e-09,
          6.44829167839589e-09,
          6.44829167839589e-09,
          5.326849628062291e-09,
          5.102561484449097e-09,
          3.784867086409349e-09,
          2.971821455588497e-09,
          6.364183402496337e-09,
          4.65398430904429e-09,
          4.289515853628245e-09,
          4.65398430904429e-09,
          4.597912273140992e-09,
          6.4202563265780555e-09,
          5.18666931625944e-09,
          4.177371781821648e-09,
          4.878272896746694e-09,
          4.794164620847141e-09,
          3.5325424807552963e-09,
          4.766128824940097e-09,
          4.177371781821648e-09,
          4.009155674111753e-09,
          3.252181857149594e-09,
          3.6727227925581474e-09,
          3.6727227925581474e-09,
          3.4484342048557437e-09,
          3.1680738032946465e-09,
          3.4203981869040945e-09,
          4.4577319613381405e-09,
          2.6353887960794964e-09,
          3.784867086409349e-09,
          3.1400377853429973e-09,
          3.3923621689524452e-09,
          2.9437854376368477e-09,
          3.252181857149594e-09,
          2.7755691078823475e-09,
          2.7755691078823475e-09,
          2.7475330899306982e-09,
          2.2428841006671973e-09,
          3.8129028823163935e-09,
          3.3082541150974976e-09,
          2.074667770912697e-09,
          2.9437854376368477e-09,
          1.9064514411581968e-09,
          2.8036051258339967e-09,
          2.467172466324996e-09,
          1.8784154232065475e-09,
          3.1680738032946465e-09,
          3.3082541150974976e-09,
          2.8036051258339967e-09,
          2.4952084842766453e-09,
          2.9157494196851985e-09,
          2.4952084842766453e-09,
          2.439136448373347e-09,
          2.186812064763899e-09,
          2.1307398068159955e-09,
          2.4952084842766453e-09,
          2.1027037888643463e-09,
          2.8877131796889444e-09,
          1.7382353334483014e-09,
          2.1307398068159955e-09,
          1.682163075500398e-09,
          2.298956136570496e-09,
          1.7662712403776482e-09,
          1.6541270575487488e-09,
          1.541982763697547e-09,
          2.4111004304216976e-09,
          1.5700188926714986e-09,
          2.298956136570496e-09,
          1.541982763697547e-09,
          1.738235222425999e-09,
          1.8223432762809466e-09,
          1.8503794052548983e-09,
          1.4018025629169983e-09
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473
         ],
         "xaxis": "x",
         "y": [
          0.0004624447028618306,
          0.0003325121651869267,
          0.0003043410833925009,
          0.0002788642013911158,
          0.00025425085914321244,
          0.00023101939586922526,
          0.0002093824150506407,
          0.0001894296146929264,
          0.00017116191156674176,
          0.00015440148126799613,
          0.0001390594698023051,
          0.0001250659697689116,
          0.00011240666935918853,
          0.00010093144373968244,
          9.052004315890372e-05,
          8.119005360640585e-05,
          7.276116230059415e-05,
          6.517731526400894e-05,
          5.856173811480403e-05,
          5.2748418966075405e-05,
          4.771667954628356e-05,
          4.336058918852359e-05,
          3.948446465074085e-05,
          3.602798096835613e-05,
          3.3043128496501595e-05,
          3.0416404115385376e-05,
          2.8127375117037445e-05,
          2.6081052055815235e-05,
          2.4285745894303545e-05,
          2.26759075303562e-05,
          2.124868842656724e-05,
          1.9967517800978385e-05,
          1.8810074834618717e-05,
          1.7779677364160307e-05,
          1.6838333976920694e-05,
          1.5986046491889283e-05,
          1.5211649042612407e-05,
          1.4495432878902648e-05,
          1.3836922335030977e-05,
          1.3232793207862414e-05,
          1.266666004084982e-05,
          1.2145886103098746e-05,
          1.1664768862829078e-05,
          1.121309742302401e-05,
          1.0787786777655128e-05,
          1.0398334779893048e-05,
          1.0034528713731561e-05,
          9.684021279099397e-06,
          9.360347576148342e-06,
          9.045936167240143e-06,
          8.748385880608112e-06,
          8.475769391225185e-06,
          8.213838555093389e-06,
          7.977081622811966e-06,
          7.739135071460623e-06,
          7.521849056502106e-06,
          7.316200026252773e-06,
          7.109363650670275e-06,
          6.922236934769899e-06,
          6.740097433066694e-06,
          6.572442998731276e-06,
          6.401464361260878e-06,
          6.246871180337621e-06,
          6.09275321039604e-06,
          5.949321348452941e-06,
          5.803514341096161e-06,
          5.671481630997732e-06,
          5.54419739273726e-06,
          5.415488431026461e-06,
          5.296752988215303e-06,
          5.175643764232518e-06,
          5.06332025906886e-06,
          4.951471964886878e-06,
          4.849359811487375e-06,
          4.7541348067170475e-06,
          4.6570094127673656e-06,
          4.561309651762713e-06,
          4.468933639145689e-06,
          4.380357040645322e-06,
          4.293206075089984e-06,
          4.203917796985479e-06,
          4.123652615817264e-06,
          4.045762580062728e-06,
          3.969059889641358e-06,
          3.889507297571981e-06,
          3.8113801110739587e-06,
          3.7463132684933953e-06,
          3.6755473047378473e-06,
          3.6138053474132903e-06,
          3.5458890579320723e-06,
          3.4793974919011816e-06,
          3.4178929126937874e-06,
          3.3575756788195577e-06,
          3.2984457902784925e-06,
          3.237653572796262e-06,
          3.1861225124885095e-06,
          3.12699285132112e-06,
          3.0752244128962047e-06,
          3.021556494786637e-06,
          2.9726379580097273e-06,
          2.917544634328806e-06,
          2.874562824217719e-06,
          2.8246943202248076e-06,
          2.7760131615650607e-06,
          2.730418827923131e-06,
          2.685537310753716e-06,
          2.6401808099763002e-06,
          2.5990982521761907e-06,
          2.554928869358264e-06,
          2.5140843717963435e-06,
          2.4760893211350776e-06,
          2.4312073492183117e-06,
          2.3922625587147195e-06,
          2.350230488445959e-06,
          2.3226841676660115e-06,
          2.2806523247709265e-06,
          2.246219082735479e-06,
          2.209886133641703e-06,
          2.1742657736467663e-06,
          2.132233703378006e-06,
          2.087589791699429e-06,
          2.0617055724869715e-06,
          2.0320219391578576e-06,
          2.0018630948470673e-06,
          1.9586436792451423e-06,
          1.930622374857194e-06,
          1.899276526273752e-06,
          1.8764795868264628e-06,
          1.849170644163678e-06,
          1.814974780245393e-06,
          1.7883783129946096e-06,
          1.759407155077497e-06,
          1.7351853784930427e-06,
          1.702651957202761e-06,
          1.6751056364228134e-06,
          1.6546832739550155e-06,
          1.6164508451765869e-06,
          1.5860548501223093e-06,
          1.558745680085849e-06,
          1.5347613953053951e-06,
          1.5176636907199281e-06,
          1.4951041293898015e-06,
          1.4742068970008404e-06,
          1.4468979543380556e-06,
          1.4326495829664054e-06,
          1.4146020248517743e-06,
          1.3825437008563313e-06,
          1.3497730151357246e-06,
          1.3322003269422567e-06,
          1.3215142189437756e-06,
          1.2984797876924858e-06,
          1.2799572459698538e-06,
          1.2585850299728918e-06,
          1.2426746707205893e-06,
          1.2208273574287887e-06,
          1.1999300113529898e-06,
          1.1842571439046878e-06,
          1.1738085277102073e-06,
          1.158848021987069e-06,
          1.1351011153237778e-06,
          1.1039927585443365e-06,
          1.0883197774091968e-06,
          1.0866574484680314e-06,
          1.0733592716860585e-06,
          1.0569738151389174e-06,
          1.0253903610646375e-06,
          9.990312719310168e-07,
          9.892951311485376e-07,
          9.729097882882343e-07,
          9.650732408772456e-07,
          9.579491688782582e-07,
          9.391891921950446e-07,
          9.168671226689185e-07,
          9.00006853044033e-07,
          8.921703624764632e-07,
          8.791095069682342e-07,
          8.698482361069182e-07,
          8.565499456381076e-07,
          8.351777864845644e-07,
          8.271038041129941e-07,
          8.085812055469432e-07,
          7.943331183923874e-07,
          7.800849743944127e-07,
          7.753355930617545e-07,
          7.57525469907705e-07,
          7.380529609690711e-07,
          7.325911610678304e-07,
          7.219051099127682e-07,
          7.154934564823634e-07,
          7.036200031507178e-07,
          6.78685807997681e-07,
          6.656250093328708e-07,
          6.556513199029723e-07,
          6.4995208504115e-07,
          6.395034688466694e-07,
          6.361788678077573e-07,
          6.216932888492011e-07,
          6.081575634198089e-07,
          6.01745909989404e-07,
          5.870228960702661e-07,
          5.851230753251002e-07,
          5.77998946482694e-07,
          5.718247280128708e-07,
          5.618510954263911e-07,
          5.38579115527682e-07,
          5.153071924723918e-07,
          5.036712309447466e-07,
          5.036712309447466e-07,
          4.946474518874311e-07,
          4.875233798884437e-07,
          4.81586653222621e-07,
          4.780246172231273e-07,
          4.7018815507726686e-07,
          4.6330154646057053e-07,
          4.597395388827863e-07,
          4.4834101231572276e-07,
          4.5190304831521644e-07,
          4.3646753056236776e-07,
          4.236442237015581e-07,
          4.146204162225331e-07,
          3.9063607459866034e-07,
          3.797125032178883e-07,
          3.773378409732686e-07,
          3.752006136892305e-07,
          3.6783907830795215e-07,
          3.6641426959249657e-07,
          3.6237730682842084e-07,
          3.590527342112182e-07,
          3.55965653398016e-07,
          3.507413453007757e-07,
          3.4646689073269954e-07,
          3.3815550182225707e-07,
          3.4100511925316823e-07,
          3.103716608165996e-07,
          2.897118633882201e-07,
          2.8781212790818245e-07,
          2.8021312914461305e-07,
          2.79738202380031e-07,
          2.7855085704686644e-07,
          2.6477766823518323e-07,
          2.616905590002716e-07,
          2.60265750284816e-07,
          2.569412060893228e-07,
          2.5052955265891796e-07,
          2.4981713409033546e-07,
          2.479173986102978e-07,
          2.462550980908418e-07,
          2.417431801404746e-07,
          2.3533151249921502e-07,
          2.3224441747515812e-07,
          2.2702010937791783e-07,
          2.3343176280832267e-07,
          2.2274566902069637e-07,
          2.084975534444311e-07,
          2.1039730313532345e-07,
          1.859379921143045e-07,
          1.728772218712038e-07,
          1.7026506782258366e-07,
          1.631409958235963e-07,
          1.5506707029544486e-07,
          1.515050342959512e-07,
          1.4889288024733105e-07,
          1.4984274798734987e-07,
          1.4224374922378047e-07,
          1.4438097650781856e-07,
          1.3535715481793886e-07,
          1.3203259641159093e-07,
          1.363070367688124e-07,
          1.310827286715721e-07,
          1.3013284672069858e-07,
          1.2752069267207844e-07,
          1.2372119329029374e-07,
          1.2063409826623683e-07,
          1.203966348839458e-07,
          1.1897181906306287e-07,
          1.1825941470533508e-07,
          1.1445991532355038e-07,
          1.1256016563265803e-07,
          1.092356072263101e-07,
          1.054361078445254e-07,
          1.0519863735680701e-07,
          1.0686091655998098e-07,
          1.0187407184503172e-07,
          9.997432215413937e-08,
          9.475001405689909e-08,
          9.261279387828836e-08,
          9.332519823601615e-08,
          8.572619947244675e-08,
          8.38264497815544e-08,
          8.358898639926338e-08,
          7.931454604204191e-08,
          7.456517181481104e-08,
          7.171554727847251e-08,
          4.654386742686256e-08,
          3.918233559829787e-08,
          3.657018154967773e-08,
          2.3509402424792825e-08,
          1.8285090774838864e-08,
          1.5910403661223427e-08,
          1.353571654760799e-08,
          1.543546623850034e-08,
          1.353571654760799e-08,
          1.2585841702161815e-08,
          8.311404897654029e-09,
          1.8997496908923495e-09,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and Validation Loss Convergence per Epoch"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"76dbb0f2-8967-4e9a-b39c-748f51c63304\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"76dbb0f2-8967-4e9a-b39c-748f51c63304\")) {                    Plotly.newPlot(                        \"76dbb0f2-8967-4e9a-b39c-748f51c63304\",                        [{\"mode\":\"lines\",\"name\":\"Train Loss\",\"type\":\"scatter\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473],\"xaxis\":\"x\",\"y\":[0.057695142924785614,0.0003822548605967313,0.00032691285014152527,0.0003015127149410546,0.00027723461971618235,0.0002535640087444335,0.0002315105957677588,0.00021212419960647821,0.0001930818980326876,0.0001766076748026535,0.0001615491055417806,0.00014708456001244485,0.0001355001440970227,0.00012400068226270378,0.00011357419134583324,0.00010452017158968374,9.55978175625205e-05,8.78597711562179e-05,8.184422040358186e-05,7.48025777284056e-05,7.023165380815044e-05,6.4332147303503e-05,5.975198291707784e-05,5.515123120858334e-05,5.1988234190503135e-05,4.8684330977266654e-05,4.555296982289292e-05,4.294889367884025e-05,4.013630677945912e-05,3.837146869045682e-05,3.637270856415853e-05,3.38662757712882e-05,3.2013049349188805e-05,3.0404768040170893e-05,2.9779406759189442e-05,2.7630758268060163e-05,2.6650706786313094e-05,2.547764415794518e-05,2.455234971421305e-05,2.359573045396246e-05,2.2562775484402664e-05,2.1785786884720437e-05,2.0949031750205904e-05,2.0239986042724922e-05,1.9259457985754125e-05,1.8840195480152033e-05,1.8262615412822925e-05,1.7358013792545535e-05,1.7065172869479284e-05,1.6252944988082163e-05,1.5548554074484855e-05,1.5780085959704593e-05,1.5134233763092197e-05,1.4454908523475751e-05,1.3959614989289548e-05,1.35086202135426e-05,1.3258478247735184e-05,1.2937402971147094e-05,1.240316305484157e-05,1.195839558931766e-05,1.163706838269718e-05,1.1232754332013428e-05,1.1318325050524436e-05,1.0874812687688973e-05,1.0652037417457905e-05,1.0275733984599356e-05,9.988695637730416e-06,9.740907444211189e-06,9.468029929848853e-06,9.39211076911306e-06,9.143285751633812e-06,8.871752470440697e-06,8.661003448651172e-06,8.712141607247759e-06,8.143058948917314e-06,7.962336894706823e-06,7.876883501012344e-06,7.679702321183868e-06,7.776233360345941e-06,7.498924333049217e-06,7.369033482973464e-06,7.188143626990495e-06,7.029737844277406e-06,6.932311862328788e-06,6.535428383358521e-06,6.413274149963399e-06,6.550652869918849e-06,6.237206434889231e-06,6.2084700402920134e-06,6.027047675161157e-06,5.941311428614426e-06,5.683013114321511e-06,5.657949714077404e-06,5.532402155949967e-06,5.341755695553729e-06,5.241218332230346e-06,5.288010925141862e-06,5.131568286742549e-06,5.116598003951367e-06,4.94994947075611e-06,4.780610652233008e-06,4.691931735578692e-06,4.637262009055121e-06,4.613402779796161e-06,4.476082267501624e-06,4.439943040779326e-06,4.414234808791662e-06,4.251287464285269e-06,4.194990651740227e-06,4.008969881397206e-06,4.08135974794277e-06,3.880899839714402e-06,3.908769031113479e-06,3.791408971665078e-06,3.68590849575412e-06,3.65086361853173e-06,3.6002579690830316e-06,3.472749313004897e-06,3.4938607313961256e-06,3.412640126043698e-06,3.3846881706267595e-06,3.322280008433154e-06,3.2101349916047184e-06,3.1489882985624718e-06,3.1838371796766296e-06,3.088934818151756e-06,3.096028422078234e-06,2.9364180136326468e-06,2.9319883196876617e-06,2.8266842946322868e-06,2.823152044584276e-06,2.813704213622259e-06,2.727605078689521e-06,2.6938489554595435e-06,2.6394031920062844e-06,2.5738261228980264e-06,2.6086756861332105e-06,2.46824174610083e-06,2.452682338116574e-06,2.441439619360608e-06,2.4063938326435164e-06,2.3799000246071955e-06,2.2528124645759817e-06,2.273026439070236e-06,2.188609414588427e-06,2.1674702566087944e-06,2.1667412966053234e-06,2.116080167979817e-06,2.101052587022423e-06,2.0422889974724967e-06,2.004776661124197e-06,1.9923845684388652e-06,1.921677949212608e-06,1.905388785417017e-06,1.8537743926572148e-06,1.8256823750562035e-06,1.7928234683495248e-06,1.8077106460623327e-06,1.7386015542797395e-06,1.7819454569689697e-06,1.7258454363400233e-06,1.6835945189086488e-06,1.6437832073279424e-06,1.6213265325859538e-06,1.603327064003679e-06,1.561160843266407e-06,1.524097001492919e-06,1.5019205648059142e-06,1.4777533579035662e-06,1.4941265362722334e-06,1.4159899137666798e-06,1.3910096186009469e-06,1.3848697335561155e-06,1.3578148809756385e-06,1.3179754887460149e-06,1.3320218386070337e-06,1.2815287391276797e-06,1.2719124242721591e-06,1.2411288707880885e-06,1.2394185660014045e-06,1.1852806665046955e-06,1.1904675147889066e-06,1.1794212468885235e-06,1.171290932688862e-06,1.1655995422188425e-06,1.1119946066173725e-06,1.0914158110608696e-06,1.0768371794256382e-06,1.0519690931687364e-06,1.0221106094832066e-06,1.0324561117158737e-06,9.890280807667295e-07,9.860002592176897e-07,9.612443818696192e-07,9.409743029209494e-07,9.268160852116125e-07,9.077516551769804e-07,9.233676792064216e-07,8.852665587255615e-07,8.650526410747261e-07,8.640432724860148e-07,8.395118129556067e-07,8.212041961996874e-07,7.988033985384391e-07,8.048871791288548e-07,7.92270952842955e-07,7.907290751063556e-07,7.395631769213651e-07,7.506935162382433e-07,7.263021188919083e-07,7.179193062256672e-07,7.121998919501493e-07,6.92630806042871e-07,6.724447985106963e-07,6.65575953462394e-07,6.609780598409998e-07,6.446330758080876e-07,6.348484475893201e-07,6.166530397422321e-07,6.088870918574685e-07,6.104010026319884e-07,5.83318126246013e-07,5.808510081806162e-07,5.795333208880038e-07,5.627117047879437e-07,5.482731353367853e-07,5.265731601866719e-07,5.239377856014471e-07,5.139849577062705e-07,5.058825536252698e-07,5.000510441277584e-07,4.861732350036618e-07,4.7103375777624024e-07,4.7582790330125135e-07,4.6769744699304283e-07,4.4728719217346224e-07,4.313066312988667e-07,4.42885550455685e-07,4.2096132801816566e-07,4.1972774056375783e-07,4.070834904723597e-07,4.1243836790272326e-07,3.841219609057589e-07,3.781502755373367e-07,3.7697276411563507e-07,3.673844446439034e-07,3.576278970740532e-07,3.549364464561222e-07,3.421519920721039e-07,3.342458398947201e-07,3.415912885884609e-07,3.228351488360204e-07,3.1086375429367763e-07,3.14312188720578e-07,3.092376630320359e-07,3.034342057617323e-07,2.9709806881328404e-07,2.781456771572266e-07,2.7741674557546503e-07,2.749495706666494e-07,2.625015724788682e-07,2.5442719220336585e-07,2.4881998683667916e-07,2.4559582811889413e-07,2.3827843165236118e-07,2.326151502529683e-07,2.257182671883129e-07,2.1955032991627377e-07,2.181765665909552e-07,2.0741072148666717e-07,2.085882329083688e-07,1.9381323568268272e-07,1.9389733552088728e-07,1.8868261975057976e-07,1.846454296128286e-07,1.8226236875307222e-07,1.7861769663340965e-07,1.7043115008164023e-07,1.691975626272324e-07,1.6605753216936137e-07,1.6064657870629162e-07,1.5851583157200366e-07,1.6000174696273461e-07,1.567776024558043e-07,1.4948822979476972e-07,1.4281565086093906e-07,1.4318011665181984e-07,1.4090919364662113e-07,1.3757289707427844e-07,1.3569449208716833e-07,1.3087228012409469e-07,1.2619027245364123e-07,1.2450810515929334e-07,1.1870464078356235e-07,1.1256474863330368e-07,1.0959292495726913e-07,1.0303249098342349e-07,9.924762167656809e-08,9.3948806068056e-08,8.579031884892174e-08,8.004292340046959e-08,7.533287060823568e-08,7.423946613016597e-08,6.809956687447993e-08,6.319326217862908e-08,5.828695037735088e-08,5.4277794703239124e-08,5.105365019630881e-08,4.735289138579901e-08,4.67360976585951e-08,4.527822383693092e-08,4.429696076613254e-08,3.857760688674716e-08,3.897011069398104e-08,3.6811336201481026e-08,3.616650445792402e-08,3.3671298638182634e-08,3.0811619922133104e-08,2.733515103159334e-08,3.033500561855362e-08,2.7447294215221518e-08,3.053126107488424e-08,2.898927675687446e-08,2.8288376086038625e-08,2.859677294964058e-08,2.3718499164715467e-08,2.6494069160776235e-08,2.2569020430296405e-08,2.5232447242728995e-08,2.545673360998535e-08,2.1447579712230436e-08,2.074667726503776e-08,2.1251326032256657e-08,2.254098596665699e-08,2.0970965408650954e-08,1.8924334099779117e-08,1.9849524690584985e-08,1.7634675941735622e-08,1.8363612852567712e-08,1.7915036565341325e-08,1.769074842172813e-08,1.8055219541679435e-08,1.6793594070918516e-08,1.735431531812992e-08,1.648519720731656e-08,1.5195539049273066e-08,1.5812334552833818e-08,1.6989847750892295e-08,1.5475899672878768e-08,1.4354458066634379e-08,1.802718152532634e-08,1.488714307384953e-08,1.216764644595969e-08,1.2840511764977691e-08,1.4466603914797815e-08,1.3569448675809781e-08,9.588330129872702e-09,1.056959142431424e-08,1.2868547116795526e-08,1.183121334236148e-08,1.1494781126941689e-08,1.2868547116795526e-08,1.1943357414168077e-08,1.0205122968898195e-08,1.1242456743332241e-08,1.1242456743332241e-08,1.0008870177102835e-08,8.999572642665044e-09,9.420113578073597e-09,8.635104187248999e-09,9.251897026274492e-09,1.0962096119726539e-08,8.410815155457385e-09,8.859393219040612e-09,7.934202628234743e-09,8.831356090865938e-09,8.999572642665044e-09,8.77528361087343e-09,9.560293889876448e-09,8.186527900022611e-09,7.261337309216742e-09,6.9249046497077416e-09,7.990274220048832e-09,8.298671083650788e-09,7.541697932822444e-09,6.420255882488846e-09,6.924905093796951e-09,7.794022316431892e-09,7.149193237410145e-09,7.093120757417637e-09,8.24259860365828e-09,6.98097668561104e-09,7.093120757417637e-09,6.728652302001592e-09,4.682020549040544e-09,5.71935432347459e-09,4.766128824940097e-09,6.476327918392144e-09,4.513804441330649e-09,6.5323999542954425e-09,5.607210251667993e-09,6.111859018886889e-09,6.44829167839589e-09,6.44829167839589e-09,5.326849628062291e-09,5.102561484449097e-09,3.784867086409349e-09,2.971821455588497e-09,6.364183402496337e-09,4.65398430904429e-09,4.289515853628245e-09,4.65398430904429e-09,4.597912273140992e-09,6.4202563265780555e-09,5.18666931625944e-09,4.177371781821648e-09,4.878272896746694e-09,4.794164620847141e-09,3.5325424807552963e-09,4.766128824940097e-09,4.177371781821648e-09,4.009155674111753e-09,3.252181857149594e-09,3.6727227925581474e-09,3.6727227925581474e-09,3.4484342048557437e-09,3.1680738032946465e-09,3.4203981869040945e-09,4.4577319613381405e-09,2.6353887960794964e-09,3.784867086409349e-09,3.1400377853429973e-09,3.3923621689524452e-09,2.9437854376368477e-09,3.252181857149594e-09,2.7755691078823475e-09,2.7755691078823475e-09,2.7475330899306982e-09,2.2428841006671973e-09,3.8129028823163935e-09,3.3082541150974976e-09,2.074667770912697e-09,2.9437854376368477e-09,1.9064514411581968e-09,2.8036051258339967e-09,2.467172466324996e-09,1.8784154232065475e-09,3.1680738032946465e-09,3.3082541150974976e-09,2.8036051258339967e-09,2.4952084842766453e-09,2.9157494196851985e-09,2.4952084842766453e-09,2.439136448373347e-09,2.186812064763899e-09,2.1307398068159955e-09,2.4952084842766453e-09,2.1027037888643463e-09,2.8877131796889444e-09,1.7382353334483014e-09,2.1307398068159955e-09,1.682163075500398e-09,2.298956136570496e-09,1.7662712403776482e-09,1.6541270575487488e-09,1.541982763697547e-09,2.4111004304216976e-09,1.5700188926714986e-09,2.298956136570496e-09,1.541982763697547e-09,1.738235222425999e-09,1.8223432762809466e-09,1.8503794052548983e-09,1.4018025629169983e-09],\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Validation Loss\",\"type\":\"scatter\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473],\"xaxis\":\"x\",\"y\":[0.0004624447028618306,0.0003325121651869267,0.0003043410833925009,0.0002788642013911158,0.00025425085914321244,0.00023101939586922526,0.0002093824150506407,0.0001894296146929264,0.00017116191156674176,0.00015440148126799613,0.0001390594698023051,0.0001250659697689116,0.00011240666935918853,0.00010093144373968244,9.052004315890372e-05,8.119005360640585e-05,7.276116230059415e-05,6.517731526400894e-05,5.856173811480403e-05,5.2748418966075405e-05,4.771667954628356e-05,4.336058918852359e-05,3.948446465074085e-05,3.602798096835613e-05,3.3043128496501595e-05,3.0416404115385376e-05,2.8127375117037445e-05,2.6081052055815235e-05,2.4285745894303545e-05,2.26759075303562e-05,2.124868842656724e-05,1.9967517800978385e-05,1.8810074834618717e-05,1.7779677364160307e-05,1.6838333976920694e-05,1.5986046491889283e-05,1.5211649042612407e-05,1.4495432878902648e-05,1.3836922335030977e-05,1.3232793207862414e-05,1.266666004084982e-05,1.2145886103098746e-05,1.1664768862829078e-05,1.121309742302401e-05,1.0787786777655128e-05,1.0398334779893048e-05,1.0034528713731561e-05,9.684021279099397e-06,9.360347576148342e-06,9.045936167240143e-06,8.748385880608112e-06,8.475769391225185e-06,8.213838555093389e-06,7.977081622811966e-06,7.739135071460623e-06,7.521849056502106e-06,7.316200026252773e-06,7.109363650670275e-06,6.922236934769899e-06,6.740097433066694e-06,6.572442998731276e-06,6.401464361260878e-06,6.246871180337621e-06,6.09275321039604e-06,5.949321348452941e-06,5.803514341096161e-06,5.671481630997732e-06,5.54419739273726e-06,5.415488431026461e-06,5.296752988215303e-06,5.175643764232518e-06,5.06332025906886e-06,4.951471964886878e-06,4.849359811487375e-06,4.7541348067170475e-06,4.6570094127673656e-06,4.561309651762713e-06,4.468933639145689e-06,4.380357040645322e-06,4.293206075089984e-06,4.203917796985479e-06,4.123652615817264e-06,4.045762580062728e-06,3.969059889641358e-06,3.889507297571981e-06,3.8113801110739587e-06,3.7463132684933953e-06,3.6755473047378473e-06,3.6138053474132903e-06,3.5458890579320723e-06,3.4793974919011816e-06,3.4178929126937874e-06,3.3575756788195577e-06,3.2984457902784925e-06,3.237653572796262e-06,3.1861225124885095e-06,3.12699285132112e-06,3.0752244128962047e-06,3.021556494786637e-06,2.9726379580097273e-06,2.917544634328806e-06,2.874562824217719e-06,2.8246943202248076e-06,2.7760131615650607e-06,2.730418827923131e-06,2.685537310753716e-06,2.6401808099763002e-06,2.5990982521761907e-06,2.554928869358264e-06,2.5140843717963435e-06,2.4760893211350776e-06,2.4312073492183117e-06,2.3922625587147195e-06,2.350230488445959e-06,2.3226841676660115e-06,2.2806523247709265e-06,2.246219082735479e-06,2.209886133641703e-06,2.1742657736467663e-06,2.132233703378006e-06,2.087589791699429e-06,2.0617055724869715e-06,2.0320219391578576e-06,2.0018630948470673e-06,1.9586436792451423e-06,1.930622374857194e-06,1.899276526273752e-06,1.8764795868264628e-06,1.849170644163678e-06,1.814974780245393e-06,1.7883783129946096e-06,1.759407155077497e-06,1.7351853784930427e-06,1.702651957202761e-06,1.6751056364228134e-06,1.6546832739550155e-06,1.6164508451765869e-06,1.5860548501223093e-06,1.558745680085849e-06,1.5347613953053951e-06,1.5176636907199281e-06,1.4951041293898015e-06,1.4742068970008404e-06,1.4468979543380556e-06,1.4326495829664054e-06,1.4146020248517743e-06,1.3825437008563313e-06,1.3497730151357246e-06,1.3322003269422567e-06,1.3215142189437756e-06,1.2984797876924858e-06,1.2799572459698538e-06,1.2585850299728918e-06,1.2426746707205893e-06,1.2208273574287887e-06,1.1999300113529898e-06,1.1842571439046878e-06,1.1738085277102073e-06,1.158848021987069e-06,1.1351011153237778e-06,1.1039927585443365e-06,1.0883197774091968e-06,1.0866574484680314e-06,1.0733592716860585e-06,1.0569738151389174e-06,1.0253903610646375e-06,9.990312719310168e-07,9.892951311485376e-07,9.729097882882343e-07,9.650732408772456e-07,9.579491688782582e-07,9.391891921950446e-07,9.168671226689185e-07,9.00006853044033e-07,8.921703624764632e-07,8.791095069682342e-07,8.698482361069182e-07,8.565499456381076e-07,8.351777864845644e-07,8.271038041129941e-07,8.085812055469432e-07,7.943331183923874e-07,7.800849743944127e-07,7.753355930617545e-07,7.57525469907705e-07,7.380529609690711e-07,7.325911610678304e-07,7.219051099127682e-07,7.154934564823634e-07,7.036200031507178e-07,6.78685807997681e-07,6.656250093328708e-07,6.556513199029723e-07,6.4995208504115e-07,6.395034688466694e-07,6.361788678077573e-07,6.216932888492011e-07,6.081575634198089e-07,6.01745909989404e-07,5.870228960702661e-07,5.851230753251002e-07,5.77998946482694e-07,5.718247280128708e-07,5.618510954263911e-07,5.38579115527682e-07,5.153071924723918e-07,5.036712309447466e-07,5.036712309447466e-07,4.946474518874311e-07,4.875233798884437e-07,4.81586653222621e-07,4.780246172231273e-07,4.7018815507726686e-07,4.6330154646057053e-07,4.597395388827863e-07,4.4834101231572276e-07,4.5190304831521644e-07,4.3646753056236776e-07,4.236442237015581e-07,4.146204162225331e-07,3.9063607459866034e-07,3.797125032178883e-07,3.773378409732686e-07,3.752006136892305e-07,3.6783907830795215e-07,3.6641426959249657e-07,3.6237730682842084e-07,3.590527342112182e-07,3.55965653398016e-07,3.507413453007757e-07,3.4646689073269954e-07,3.3815550182225707e-07,3.4100511925316823e-07,3.103716608165996e-07,2.897118633882201e-07,2.8781212790818245e-07,2.8021312914461305e-07,2.79738202380031e-07,2.7855085704686644e-07,2.6477766823518323e-07,2.616905590002716e-07,2.60265750284816e-07,2.569412060893228e-07,2.5052955265891796e-07,2.4981713409033546e-07,2.479173986102978e-07,2.462550980908418e-07,2.417431801404746e-07,2.3533151249921502e-07,2.3224441747515812e-07,2.2702010937791783e-07,2.3343176280832267e-07,2.2274566902069637e-07,2.084975534444311e-07,2.1039730313532345e-07,1.859379921143045e-07,1.728772218712038e-07,1.7026506782258366e-07,1.631409958235963e-07,1.5506707029544486e-07,1.515050342959512e-07,1.4889288024733105e-07,1.4984274798734987e-07,1.4224374922378047e-07,1.4438097650781856e-07,1.3535715481793886e-07,1.3203259641159093e-07,1.363070367688124e-07,1.310827286715721e-07,1.3013284672069858e-07,1.2752069267207844e-07,1.2372119329029374e-07,1.2063409826623683e-07,1.203966348839458e-07,1.1897181906306287e-07,1.1825941470533508e-07,1.1445991532355038e-07,1.1256016563265803e-07,1.092356072263101e-07,1.054361078445254e-07,1.0519863735680701e-07,1.0686091655998098e-07,1.0187407184503172e-07,9.997432215413937e-08,9.475001405689909e-08,9.261279387828836e-08,9.332519823601615e-08,8.572619947244675e-08,8.38264497815544e-08,8.358898639926338e-08,7.931454604204191e-08,7.456517181481104e-08,7.171554727847251e-08,4.654386742686256e-08,3.918233559829787e-08,3.657018154967773e-08,2.3509402424792825e-08,1.8285090774838864e-08,1.5910403661223427e-08,1.353571654760799e-08,1.543546623850034e-08,1.353571654760799e-08,1.2585841702161815e-08,8.311404897654029e-09,1.8997496908923495e-09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Training and Validation Loss Convergence per Epoch\"},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('76dbb0f2-8967-4e9a-b39c-748f51c63304');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Training Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473
         ],
         "xaxis": "x",
         "y": [
          0.9698964953422546,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines",
         "name": "Validation Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473
         ],
         "xaxis": "x",
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and Validation Accuracy Convergence per Epoch"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Accuracy"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"7017b7a8-f7b2-42a2-ac5a-fe897aa8ae9a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7017b7a8-f7b2-42a2-ac5a-fe897aa8ae9a\")) {                    Plotly.newPlot(                        \"7017b7a8-f7b2-42a2-ac5a-fe897aa8ae9a\",                        [{\"mode\":\"lines\",\"name\":\"Training Accuracy\",\"type\":\"scatter\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473],\"xaxis\":\"x\",\"y\":[0.9698964953422546,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"Validation Accuracy\",\"type\":\"scatter\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473],\"xaxis\":\"x\",\"y\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"yaxis\":\"y\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Training and Validation Accuracy Convergence per Epoch\"},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracy\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('7017b7a8-f7b2-42a2-ac5a-fe897aa8ae9a');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_convergence(history):\n",
    "    epochs = []\n",
    "    for i in range(len(history.history['loss'])):\n",
    "        epochs.append(i)\n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    fig1 = make_subplots(rows=1, cols=1, specs=[[{'type':'xy'}]])\n",
    "    fig1.add_trace(go.Scatter(x=epochs, y=train_loss, mode=\"lines\", name=\"Train Loss\"), row=1, col=1)\n",
    "    fig1.add_trace(go.Scatter(x=epochs, y=val_loss, mode=\"lines\", name=\"Validation Loss\"), row=1, col=1)\n",
    "    fig1.update_layout(\n",
    "        title = \"Training and Validation Loss Convergence per Epoch\", \n",
    "        xaxis1 = dict(title_text = \"Epoch\"),\n",
    "        yaxis1 = dict(title_text = \"Loss\"),\n",
    "    )\n",
    "#     fig1.write_image()\n",
    "    fig1.show()\n",
    "    \n",
    "    train_acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    fig2 = make_subplots(rows=1, cols=1, specs=[[{'type':'xy'}]])\n",
    "    fig2.add_trace(go.Scatter(x=epochs, y=train_acc, mode=\"lines\", name=\"Training Accuracy\"), row=1, col=1)\n",
    "    fig2.add_trace(go.Scatter(x=epochs, y=val_acc, mode=\"lines\", name=\"Validation Accuracy\"), row=1, col=1)\n",
    "    fig2.update_layout(\n",
    "        title = \"Training and Validation Accuracy Convergence per Epoch\", \n",
    "        xaxis1 = dict(title_text = \"Epoch\"),\n",
    "        yaxis1 = dict(title_text = \"Accuracy\"),\n",
    "    )\n",
    "#     fig2.write_image()\n",
    "    fig2.show()\n",
    "    \n",
    "evaluate_convergence(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set evaluation\n",
      "\tAccuracy: 1.0\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 1.0\n",
      "\tF1-score: 1.0\n",
      "Validation set evaluation\n",
      "\tAccuracy: 1.0\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 1.0\n",
      "\tF1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, x, y):\n",
    "    y = y.tolist()\n",
    "    y_pred = model.predict(x)\n",
    "    y_pred = y_pred.round()\n",
    "    y_pred = y_pred.tolist()\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        y[i] = int(y[i][0])\n",
    "        y_pred[i] = int(y_pred[i][0][0])\n",
    "    \n",
    "    acc = accuracy(y, y_pred)\n",
    "    print(f\"\\tAccuracy: {acc}\")\n",
    "    \n",
    "    prec = precision(y, y_pred)\n",
    "    print(f\"\\tPrecision: {prec}\")\n",
    "    \n",
    "    rec = recall(y, y_pred)\n",
    "    print(f\"\\tRecall: {rec}\")\n",
    "    \n",
    "    f = f1(y, y_pred)\n",
    "    print(f\"\\tF1-score: {f}\")\n",
    "    \n",
    "print(\"Training set evaluation\")\n",
    "evaluate(model, x_train, y_train)\n",
    "print(\"Validation set evaluation\")\n",
    "evaluate(model, x_val, y_val)\n",
    "# print(\"Test set evaluation\")\n",
    "# evaluate(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def confusion_matrix(model, x, y):\n",
    "    y_pred = model.predict(x, batch_size=1)\n",
    "\n",
    "    originals = []\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        originals.append(y[i][0])\n",
    "        predictions.append(round(float(y_pred[i][0])))\n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['Actual'] = originals\n",
    "    df['Predicted'] = predictions\n",
    "    \n",
    "    conf_matrix = pd.crosstab(df['Actual'], df['Predicted'])\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix of Actual versus Predicted Labels\")\n",
    "    plt.show()\n",
    "    \n",
    "# confusion_matrix(model, x_train, y_train)\n",
    "confusion_matrix(model, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jr/swvxxh453l3_0psw7xssft500000gn/T/ipykernel_82914/4162026297.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Dataset v3/Indices/S&P 500 Historical Data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mcandles_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandles_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_candles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mplot_predictions_candles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandles_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0mplot_predictions_candles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandles_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jr/swvxxh453l3_0psw7xssft500000gn/T/ipykernel_82914/4162026297.py\u001b[0m in \u001b[0;36mplot_predictions_candles\u001b[0;34m(x, y, df, dataset)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#         real_label = row[1][7]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#         pred_label = row[1][8]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mreal_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def retrieve_candles(filename, val_year, test_year):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df.columns = [\"Date\", \"Close\", \"Open\", \"High\", \"Low\", \"Vol.\", \"Change%\"]\n",
    "    df = df.sort_values('Date', ascending=True)\n",
    "    \n",
    "    candles_val = df[df['Date'].dt.year == val_year]\n",
    "    candles_train = df[df['Date'].dt.year < val_year]\n",
    "    candles_train = candles_train[candles_train['Date'] > datetime(2009,7,10)]\n",
    "    \n",
    "    candles_train = candles_train.replace(',','', regex=True)\n",
    "    candles_val = candles_val.replace(',','', regex=True)\n",
    "\n",
    "    return candles_train, candles_val\n",
    "\n",
    "def plot_predictions_candles(x, y, df, dataset):\n",
    "    # PREDICTIONS LIJKEN NOG NIET GOED UITGELIJND MET CANDLES???\n",
    "    originals = []\n",
    "    predictions = []\n",
    "    y_pred = model.predict(x)\n",
    "    for i in range(len(y_pred)):\n",
    "        originals.append(np.argmax(y[i]))\n",
    "        predictions.append(np.argmax(y_pred[i]))\n",
    "    \n",
    "    for i in range(len(originals)):\n",
    "        if originals[i] == 0:\n",
    "            originals[i] = -2\n",
    "        elif originals[i] == 1:\n",
    "            originals[i] = -1\n",
    "        elif originals[i] == 2:\n",
    "            originals[i] = 1\n",
    "        elif originals[i] == 3:\n",
    "            originals[i] = 2\n",
    "        \n",
    "        if predictions[i] == 0:\n",
    "            predictions[i] = -2\n",
    "        elif predictions[i] == 1:\n",
    "            predictions[i] = -1\n",
    "        elif predictions[i] == 2:\n",
    "            predictions[i] = 1\n",
    "        elif predictions[i] == 3:\n",
    "            predictions[i] = 2\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Candlestick(x=df['Date'], open=df['Open'], high=df['High'], low=df['Low'], close=df['Close'], name=\"Price\"))\n",
    "    standard_factor = 0.5 * mean([float(df.iloc[0].tolist()[3]), float(df.iloc[0].tolist()[4])])\n",
    "    \n",
    "    green_legend = False\n",
    "    orange_legend = False\n",
    "    red_legend = False\n",
    "    \n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        pred_date = row[1][0].to_pydatetime()\n",
    "        pred_high = float(row[1][3])\n",
    "        pred_low = float(row[1][4])\n",
    "#         real_label = row[1][7]\n",
    "#         pred_label = row[1][8]\n",
    "        real_label = originals[i]\n",
    "        pred_label = predictions[i]\n",
    "        \n",
    "        start_shape = pred_date - timedelta(days=0.1)\n",
    "        end_shape = pred_date + timedelta(days=0.1)\n",
    "        \n",
    "        if pred_label == 1:\n",
    "            extreme = pred_high + 0.1 * standard_factor\n",
    "            standard = pred_high + 0.05 * standard_factor\n",
    "        elif pred_label == 2:\n",
    "            extreme = pred_high + 0.2 * standard_factor\n",
    "            standard = pred_high + 0.05 * standard_factor\n",
    "        elif pred_label == -1:\n",
    "            extreme = pred_low - 0.1 * standard_factor\n",
    "            standard = pred_low -  0.05 * standard_factor\n",
    "        elif pred_label == -2:\n",
    "            extreme = pred_low - 0.2 * standard_factor\n",
    "            standard = pred_low - 0.05 * standard_factor\n",
    "            \n",
    "        if real_label == pred_label:\n",
    "            color = \"green\"\n",
    "        elif np.sign(real_label) == np.sign(pred_label):\n",
    "            color = \"orange\"\n",
    "        else:\n",
    "            color = \"red\"\n",
    "        \n",
    "        if real_label == pred_label and not green_legend:\n",
    "            green_legend = True\n",
    "            fig.add_trace(go.Scatter(x=[start_shape,pred_date,end_shape,start_shape], \n",
    "                                 y=[standard,extreme,standard,standard], \n",
    "                                 fill=\"toself\", name=\"Correct Direction / Correct Magnitude\", mode=\"lines\", line=dict(color=color)))\n",
    "        elif np.sign(real_label) == np.sign(pred_label) and not orange_legend:\n",
    "            orange_legend = True\n",
    "            fig.add_trace(go.Scatter(x=[start_shape,pred_date,end_shape,start_shape], \n",
    "                                 y=[standard,extreme,standard,standard], \n",
    "                                 fill=\"toself\", name=\"Correct Direction / Inorrect Magnitude\", mode=\"lines\", line=dict(color=color)))\n",
    "        elif np.sign(real_label) != np.sign(pred_label) and not red_legend:\n",
    "            red_legend = True\n",
    "            fig.add_trace(go.Scatter(x=[start_shape,pred_date,end_shape,start_shape], \n",
    "                                 y=[standard,extreme,standard,standard], \n",
    "                                 fill=\"toself\", name=\"Incorrect Direction\", mode=\"lines\", line=dict(color=color)))\n",
    "        else:\n",
    "            fig.add_trace(go.Scatter(x=[start_shape,pred_date,end_shape,start_shape], \n",
    "                                 y=[standard,extreme,standard,standard], \n",
    "                                 fill=\"toself\", showlegend=False, mode=\"lines\", line=dict(color=color)))\n",
    "    \n",
    "    title = f\"Visualization of {dataset} Predictions\"\n",
    "    fig.update_xaxes(title_text=\"Date\")\n",
    "    fig.update_yaxes(title_text=\"Price\")\n",
    "    layout = dict(title=title, height=800, width=1500)\n",
    "    fig.update_layout(layout)\n",
    "#     config = dict({'scrollZoom': True})\n",
    "#     fig.show(config=config)\n",
    "    fig.show()\n",
    "\n",
    "filename = \"Dataset v3/Indices/S&P 500 Historical Data.csv\"\n",
    "candles_train, candles_val = retrieve_candles(filename, val_year, test_year) \n",
    "plot_predictions_candles(x_train, y_train, candles_train, \"Training\")\n",
    "plot_predictions_candles(x_val, y_val, candles_val, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
